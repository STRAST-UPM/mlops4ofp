{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# F07 - Deploy & Runtime Validation (autocontenido)\n\nEste notebook implementa completamente la Fase 07 sin depender de `scripts/07_deployrun.py`.\n\nFlujo:\n1) prepare: genera `manifest.json` a partir de la variante F06 padre\n2) run: arranca servidor Flask (minimo), ejecuta cliente batch, guarda logs crudos (parquet+csv)\n3) postprocess: calcula metricas por modelo (`prediction_name`) incluyendo recuentos `no_ref_*`\n4) report: genera `report.html` y figuras en `report/figures/`\n5) traceability: escribe `07_deployrun_metadata.json`\n\nPropiedades:\n- Idempotente: al ejecutar run se regeneran `runtime/ logs/ metrics/ report/`.\n- Caja negra: servidor+cliente se ejecutan dentro del notebook como sistema de prueba.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===============================\n# Configuracion\n# ===============================\nVARIANT = \"v001\"   # <-- cambia por tu variante F07 (vNNN)\n\nDEFAULT_HOST = \"127.0.0.1\"\nDEFAULT_PORT = 5005\n\nprint(f\"[F07] VARIANT = {VARIANT}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===============================\n# Bootstrap: localizar project root\n# ===============================\nfrom pathlib import Path\nimport sys\n\nROOT = Path().resolve()\nfor _ in range(10):\n    if (ROOT / \"mlops4ofp\").exists():\n        break\n    ROOT = ROOT.parent\nelse:\n    raise RuntimeError(\"No se pudo localizar project root (carpeta mlops4ofp)\")\n\nsys.path.insert(0, str(ROOT))\nprint(\"[F07] project_root =\", ROOT)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===============================\n# Imports (proyecto + deps)\n# ===============================\nimport json\nimport shutil\nimport time\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom threading import Thread\n\nimport numpy as np\nimport pandas as pd\nimport requests\nimport yaml\n\nimport tensorflow as tf\nfrom flask import Flask, request, jsonify\nfrom werkzeug.serving import make_server\n\nimport matplotlib.pyplot as plt\n\nfrom mlops4ofp.tools.params_manager import ParamsManager\nfrom mlops4ofp.tools.run_context import detect_execution_dir, detect_project_root\nfrom mlops4ofp.tools.traceability import write_metadata\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===============================\n# Paths y utilidades\n# ===============================\nPHASE = \"07_deployrun\"\n\ndef ensure_clean_dir(path: Path):\n    if path.exists():\n        shutil.rmtree(path)\n    path.mkdir(parents=True, exist_ok=True)\n\ndef variant_root_from_pm(variant: str) -> Path:\n    execution_dir = detect_execution_dir()\n    project_root = detect_project_root(execution_dir)\n    pm = ParamsManager(PHASE, project_root)\n    pm.set_current(variant)\n    return pm.current_variant_dir()\n\nvariant_root = variant_root_from_pm(VARIANT)\nprint(\"[F07] variant_root =\", variant_root)\n\nparams_path = variant_root / \"params.yaml\"\nif not params_path.exists():\n    raise FileNotFoundError(f\"No existe params.yaml para {PHASE}:{VARIANT} en {params_path}\")\n\nwith open(params_path, \"r\", encoding=\"utf-8\") as f:\n    f07_params = yaml.safe_load(f)\n\nparent_f06 = f07_params.get(\"parent_variant_f06\")\nif not parent_f06:\n    raise ValueError(\"parent_variant_f06 debe estar definido en params.yaml de F07\")\n\nruntime_cfg = (f07_params.get(\"runtime\") or {})\nHOST = runtime_cfg.get(\"host\", DEFAULT_HOST)\nPORT = int(runtime_cfg.get(\"port\", DEFAULT_PORT))\n\nprint(\"[F07] parent_f06 =\", parent_f06)\nprint(\"[F07] runtime =\", HOST, PORT)\n\nruntime_dir = variant_root / \"runtime\"\nlogs_dir = variant_root / \"logs\"\nmetrics_dir = variant_root / \"metrics\"\nreport_dir = variant_root / \"report\"\nfigures_dir = report_dir / \"figures\"\n\nmanifest_path = variant_root / \"manifest.json\"\nmetadata_path = variant_root / f\"{PHASE}_metadata.json\"\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Prepare - generar manifest.json\n\n`manifest.json` es el contrato sellado de F07:\n- lista de modelos (por `prediction_name`)\n- directorio del modelo en el paquete F06 (con `model.h5` y `model_summary.json`)\n- dataset asociado (parquet F04 copiado en F06)\n- columnas (`OW_events`, `label`)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===============================\n# Prepare: construir manifest.json\n# ===============================\nproject_root = ROOT\nf06_root = project_root / \"executions\" / \"06_packaging\" / parent_f06\nif not f06_root.exists():\n    raise FileNotFoundError(f\"No existe paquete F06: {f06_root}\")\n\nf06_metadata_path = f06_root / \"06_packaging_metadata.json\"\nif not f06_metadata_path.exists():\n    candidates = list(f06_root.glob(\"*_metadata.json\"))\n    if not candidates:\n        raise FileNotFoundError(f\"No se encontro metadata F06 en {f06_root}\")\n    f06_metadata_path = candidates[0]\n\nf06_metadata = json.loads(f06_metadata_path.read_text(encoding=\"utf-8\"))\n\nmodels_manifest = []\ndatasets_manifest = []\n\ndatasets_dir_f06 = f06_root / \"datasets\"\nmodels_dir_f06 = f06_root / \"models\"\n\nseen_datasets = set()\n\nfor m in f06_metadata.get(\"models\", []):\n    pred_name = m[\"prediction_name\"]\n    v05 = m[\"source_f05\"]\n\n    model_candidates = list(models_dir_f06.glob(f\"{pred_name}__*\"))\n    if len(model_candidates) != 1:\n        raise RuntimeError(\n            f\"Esperaba 1 directorio para modelo '{pred_name}' en {models_dir_f06}, encontrado: {model_candidates}\"\n        )\n    model_dir = model_candidates[0]\n    model_h5 = model_dir / \"model.h5\"\n    model_summary = model_dir / \"model_summary.json\"\n    if not model_h5.exists():\n        raise FileNotFoundError(f\"No existe model.h5 en {model_dir}\")\n    if not model_summary.exists():\n        raise FileNotFoundError(f\"No existe model_summary.json en {model_dir} (necesario para vectorizacion runtime)\")\n\n    f05_params_path = project_root / \"executions\" / \"05_modeling\" / v05 / \"params.yaml\"\n    if not f05_params_path.exists():\n        raise FileNotFoundError(f\"No existe params.yaml de F05 {v05}: {f05_params_path}\")\n    f05_params = yaml.safe_load(f05_params_path.read_text(encoding=\"utf-8\"))\n    v04 = f05_params[\"parent_variant\"]\n\n    dataset_path = datasets_dir_f06 / f\"{v04}__dataset.parquet\"\n    if not dataset_path.exists():\n        raise FileNotFoundError(f\"No existe dataset F04 copiado en F06: {dataset_path}\")\n\n    models_manifest.append({\n        \"prediction_name\": pred_name,\n        \"source_f05\": v05,\n        \"source_f04\": v04,\n        \"model_dir\": str(model_dir),\n        \"model_h5\": \"model.h5\",\n        \"model_summary\": \"model_summary.json\",\n        \"dataset_path\": str(dataset_path),\n        \"x_column\": \"OW_events\",\n        \"y_column\": \"label\",\n    })\n\n    if str(dataset_path) not in seen_datasets:\n        datasets_manifest.append({\n            \"dataset_path\": str(dataset_path),\n            \"x_column\": \"OW_events\",\n            \"y_column\": \"label\",\n            \"source_f04\": v04,\n        })\n        seen_datasets.add(str(dataset_path))\n\nmanifest = {\n    \"phase\": PHASE,\n    \"variant\": VARIANT,\n    \"f06_variant\": parent_f06,\n    \"f06_path\": str(f06_root),\n    \"created_at\": datetime.now(timezone.utc).isoformat(),\n    \"runtime\": {\"host\": HOST, \"port\": PORT},\n    \"models\": models_manifest,\n    \"datasets\": datasets_manifest,\n}\n\nmanifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\nprint(\"[OK] manifest.json generado en:\", manifest_path)\nprint(\"[OK] modelos:\", len(models_manifest), \"| datasets:\", len(datasets_manifest))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Run - servidor + cliente (batch) + logs crudos\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===============================\n# Servidor Flask en thread (para notebook)\n# ===============================\nclass ServerThread(Thread):\n    def __init__(self, app, host, port):\n        super().__init__(daemon=True)\n        self.server = make_server(host, port, app)\n        self.ctx = app.app_context()\n        self.ctx.push()\n\n    def run(self):\n        self.server.serve_forever()\n\n    def shutdown(self):\n        self.server.shutdown()\n\n\ndef build_runtime_server(manifest: dict) -> Flask:\n    app = Flask(__name__)\n\n    loaded_models = []\n    for m in manifest[\"models\"]:\n        model_dir = Path(m[\"model_dir\"])\n        summary = json.loads((model_dir / m[\"model_summary\"]).read_text(encoding=\"utf-8\"))\n        model = tf.keras.models.load_model(model_dir / m[\"model_h5\"])\n\n        loaded_models.append({\n            \"prediction_name\": summary[\"prediction_name\"],\n            \"model\": model,\n            \"vectorization\": summary[\"vectorization\"],\n            \"threshold\": float(summary.get(\"threshold\", 0.5)),\n        })\n\n    def vectorize_dense_bow(window, cfg):\n        vocab = cfg[\"vocab\"]\n        input_dim = int(cfg[\"input_dim\"])\n        index = {int(ev): i for i, ev in enumerate(vocab)}\n        X = np.zeros((1, input_dim), dtype=np.float32)\n        for ev in window:\n            i = index.get(int(ev))\n            if i is not None:\n                X[0, i] += 1.0\n        return X\n\n    def vectorize_sequence(window, cfg):\n        vocab = cfg[\"vocab\"]\n        max_len = int(cfg[\"max_len\"])\n        index = {int(ev): i + 1 for i, ev in enumerate(vocab)}\n        seq = [index[int(e)] for e in window if int(e) in index]\n        seq = seq[-max_len:]\n        X = np.zeros((1, max_len), dtype=np.int32)\n        if len(seq) > 0:\n            X[0, -len(seq):] = np.array(seq, dtype=np.int32)\n        return X\n\n    def vectorize(window, cfg):\n        vtype = cfg.get(\"vectorization\")\n        if vtype == \"dense_bow\":\n            return vectorize_dense_bow(window, cfg)\n        if vtype == \"sequence\":\n            return vectorize_sequence(window, cfg)\n        raise ValueError(f\"Vectorization no soportada: {vtype}\")\n\n    @app.route(\"/infer\", methods=[\"POST\"])\n    def infer():\n        payload = request.get_json(force=True)\n        window = payload[\"window\"]\n        results = []\n        for m in loaded_models:\n            X = vectorize(window, m[\"vectorization\"])\n            y_prob = float(m[\"model\"].predict(X, verbose=0).ravel()[0])\n            y_pred = int(y_prob >= m[\"threshold\"])\n            results.append({\"prediction_name\": m[\"prediction_name\"], \"y_pred\": y_pred})\n        return jsonify({\"window\": window, \"results\": results})\n\n    @app.route(\"/control\", methods=[\"POST\"])\n    def control():\n        payload = request.get_json(force=True)\n        if payload.get(\"cmd\") == \"shutdown\":\n            return jsonify({\"status\": \"shutting_down\"})\n        return jsonify({\"status\": \"unknown_command\"})\n\n    return app\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===============================\n# Run: orquestacion idempotente\n# ===============================\nmanifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n\nensure_clean_dir(runtime_dir)\nensure_clean_dir(logs_dir)\nensure_clean_dir(metrics_dir)\nensure_clean_dir(report_dir)\nfigures_dir.mkdir(parents=True, exist_ok=True)\n\napp = build_runtime_server(manifest)\nserver_thread = ServerThread(app, HOST, PORT)\nserver_thread.start()\n\nbase_url = f\"http://{HOST}:{PORT}\"\nfor _ in range(50):\n    try:\n        r = requests.post(f\"{base_url}/infer\", json={\"window\": []}, timeout=2)\n        if r.status_code == 200:\n            break\n    except Exception:\n        time.sleep(0.1)\nelse:\n    server_thread.shutdown()\n    raise RuntimeError(\"El servidor no ha arrancado correctamente\")\n\nraw_rows = []\nfor ds in manifest[\"datasets\"]:\n    df = pd.read_parquet(ds[\"dataset_path\"])\n    xcol = ds[\"x_column\"]\n    for _, row in df.iterrows():\n        window = row[xcol]\n        resp = requests.post(f\"{base_url}/infer\", json={\"window\": window}, timeout=30)\n        resp.raise_for_status()\n        data = resp.json()\n        window_str = json.dumps(data[\"window\"], separators=(\",\", \":\"), ensure_ascii=False)\n        for rr in data[\"results\"]:\n            raw_rows.append({\n                \"window\": window_str,\n                \"prediction_name\": rr[\"prediction_name\"],\n                \"y_pred\": int(rr[\"y_pred\"]),\n            })\n\nraw_df = pd.DataFrame(raw_rows)\nraw_parquet_path = logs_dir / \"raw_predictions.parquet\"\nraw_csv_path = logs_dir / \"raw_predictions.csv\"\nraw_df.to_parquet(raw_parquet_path, index=False)\nraw_df.to_csv(raw_csv_path, index=False)\n\ntry:\n    requests.post(f\"{base_url}/control\", json={\"cmd\": \"shutdown\"}, timeout=5)\nfinally:\n    server_thread.shutdown()\n\nprint(\"[OK] logs crudos guardados:\")\nprint(\" -\", raw_parquet_path)\nprint(\" -\", raw_csv_path)\nprint(\"[OK] filas:\", len(raw_df))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Postprocess - metricas por modelo (`prediction_name`) + `no_ref_*`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\nraw_df = pd.read_parquet(raw_parquet_path)\n\n# Indice rapido: (prediction_name, window_str) -> y_pred (primera aparicion)\npred_map = {}\nfor row in raw_df.itertuples(index=False):\n    key = (row.prediction_name, row.window)\n    if key not in pred_map:\n        pred_map[key] = int(row.y_pred)\n\nmetrics_rows = []\n\nfor m in manifest[\"models\"]:\n    pred_name = m[\"prediction_name\"]\n    dataset_path = Path(m[\"dataset_path\"])\n    xcol = m[\"x_column\"]\n    ycol = m[\"y_column\"]\n\n    df = pd.read_parquet(dataset_path)\n\n    ref_windows = set()\n    tp = tn = fp = fn = 0\n\n    for row in df.itertuples(index=False):\n        window = getattr(row, xcol)\n        y_true = int(getattr(row, ycol))\n        window_str = json.dumps(window, separators=(\",\", \":\"), ensure_ascii=False)\n        ref_windows.add(window_str)\n\n        y_pred = pred_map.get((pred_name, window_str))\n        if y_pred is None:\n            continue\n\n        if y_true == 1 and y_pred == 1:\n            tp += 1\n        elif y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n\n    model_preds = raw_df[raw_df[\"prediction_name\"] == pred_name]\n    no_ref_pred_1 = int(((~model_preds[\"window\"].isin(ref_windows)) & (model_preds[\"y_pred\"] == 1)).sum())\n    no_ref_pred_0 = int(((~model_preds[\"window\"].isin(ref_windows)) & (model_preds[\"y_pred\"] == 0)).sum())\n    no_ref_total = int(no_ref_pred_0 + no_ref_pred_1)\n\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n    recall = tp / (tp + fn) if (tp + fn) else 0.0\n    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n\n    metrics_rows.append({\n        \"prediction_name\": pred_name,\n        \"source_f05\": m[\"source_f05\"],\n        \"source_f04\": m[\"source_f04\"],\n        \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"no_ref_pred_1\": no_ref_pred_1,\n        \"no_ref_pred_0\": no_ref_pred_0,\n        \"no_ref_total\": no_ref_total,\n    })\n\n    cm = np.array([[tn, fp], [fn, tp]], dtype=int)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(cm)\n    plt.title(f\"Confusion - {pred_name}\")\n    plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n    plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n    for (i, j), v in np.ndenumerate(cm):\n        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n    plt.tight_layout()\n    plt.savefig(figures_dir / f\"confusion_{pred_name}.png\")\n    plt.close()\n\nmetrics_df = pd.DataFrame(metrics_rows)\nmetrics_csv_path = metrics_dir / \"metrics_per_model.csv\"\nmetrics_df.to_csv(metrics_csv_path, index=False)\n\nprint(\"[OK] metricas guardadas:\", metrics_csv_path)\nmetrics_df\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Report - HTML + figuras (`report/figures/`)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def html_escape(s: str) -> str:\n    return (\n        s.replace(\"&\", \"&amp;\")\n         .replace(\"<\", \"&lt;\")\n         .replace(\">\", \"&gt;\")\n         .replace('\"', \"&quot;\")\n         .replace(\"'\", \"&#39;\")\n    )\n\nrows_html = metrics_df.to_html(index=False)\n\nimgs = []\nfor m in manifest[\"models\"]:\n    pred_name = m[\"prediction_name\"]\n    img_rel = f\"figures/confusion_{pred_name}.png\"\n    img_path = figures_dir / f\"confusion_{pred_name}.png\"\n    if img_path.exists():\n        imgs.append(f\"<h3>{html_escape(pred_name)}</h3><img src='{img_rel}' style='max-width:420px;'/>\")\n\nreport_html = f\"\"\"<!doctype html>\n<html>\n<head>\n  <meta charset=\"utf-8\"/>\n  <title>F07 Report - {html_escape(VARIANT)}</title>\n  <style>\n    body {{ font-family: Arial, sans-serif; margin: 24px; }}\n    table {{ border-collapse: collapse; }}\n    th, td {{ border: 1px solid #ddd; padding: 6px 10px; }}\n    th {{ background: #f3f3f3; }}\n    code {{ background:#f7f7f7; padding:2px 4px; }}\n  </style>\n</head>\n<body>\n  <h1>F07 - Deploy & Runtime Validation</h1>\n  <p><b>Variant:</b> <code>{html_escape(VARIANT)}</code></p>\n  <p><b>Parent F06:</b> <code>{html_escape(parent_f06)}</code></p>\n  <p><b>Generated:</b> {datetime.now(timezone.utc).isoformat()}</p>\n\n  <h2>Metrics per model</h2>\n  {rows_html}\n\n  <h2>Confusion matrices</h2>\n  {''.join(imgs)}\n</body>\n</html>\"\"\"\n\nreport_path = report_dir / \"report.html\"\nreport_path.write_text(report_html, encoding=\"utf-8\")\nprint(\"[OK] report:\", report_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Trazabilidad - `07_deployrun_metadata.json`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "write_metadata(\n    stage=PHASE,\n    variant=VARIANT,\n    parent_variant=parent_f06,\n    inputs=[str(manifest_path)],\n    outputs=[str(logs_dir), str(metrics_dir), str(report_dir)],\n    params=f07_params,\n    metadata_path=metadata_path,\n)\n\nprint(\"[OK] metadata:\", metadata_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Resumen de artefactos generados\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "print(\"== Artefactos F07 ==\")\nprint(\"manifest :\", manifest_path)\nprint(\"logs     :\", raw_parquet_path, raw_csv_path)\nprint(\"metrics  :\", metrics_csv_path)\nprint(\"report   :\", report_path)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}