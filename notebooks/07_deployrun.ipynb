{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# F07 - Deploy & Runtime Validation (autocontenido)\n",
        "\n",
        "Este notebook implementa completamente la Fase 07 sin depender de `scripts/07_deployrun.py`.\n",
        "\n",
        "Flujo:\n",
        "1) prepare: genera `manifest.json` a partir de la variante F06 padre\n",
        "2) run: arranca servidor Flask (minimo), ejecuta cliente batch, guarda logs crudos (parquet+csv)\n",
        "3) postprocess: calcula metricas por modelo (`prediction_name`) incluyendo recuentos `no_ref_*`\n",
        "4) report: genera `report.html` y figuras en `report/figures/`\n",
        "5) traceability: escribe `07_deployrun_metadata.json`\n",
        "\n",
        "Propiedades:\n",
        "- Idempotente: al ejecutar run se regeneran `runtime/ logs/ metrics/ report/`.\n",
        "- Caja negra: servidor+cliente se ejecutan dentro del notebook como sistema de prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f66ce57f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Configuracion\n",
        "# ===============================\n",
        "import os\n",
        "\n",
        "VARIANT = os.getenv(\"ACTIVE_VARIANT\", \"v701\")   # <-- cambia por tu variante F07 (vNNN)\n",
        "\n",
        "DEFAULT_HOST = \"127.0.0.1\"\n",
        "DEFAULT_PORT = 5005\n",
        "\n",
        "print(f\"[F07] VARIANT = {VARIANT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b2942f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Bootstrap: localizar project root\n",
        "# ===============================\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "ROOT = Path().resolve()\n",
        "for _ in range(10):\n",
        "    if (ROOT / \"mlops4ofp\").exists():\n",
        "        break\n",
        "    ROOT = ROOT.parent\n",
        "else:\n",
        "    raise RuntimeError(\"No se pudo localizar project root (carpeta mlops4ofp)\")\n",
        "\n",
        "sys.path.insert(0, str(ROOT))\n",
        "print(\"[F07] project_root =\", ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278c6414",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Imports (proyecto + deps)\n",
        "# ===============================\n",
        "import json\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from threading import Thread\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import yaml\n",
        "\n",
        "import tensorflow as tf\n",
        "from flask import Flask, request, jsonify\n",
        "from werkzeug.serving import make_server\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mlops4ofp.tools.params_manager import ParamsManager\n",
        "from mlops4ofp.tools.run_context import detect_execution_dir, detect_project_root\n",
        "from mlops4ofp.tools.traceability import write_metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99afa758",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Paths y utilidades\n",
        "# ===============================\n",
        "PHASE = \"07_deployrun\"\n",
        "\n",
        "def ensure_clean_dir(path: Path):\n",
        "    if path.exists():\n",
        "        shutil.rmtree(path)\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def variant_root_from_pm(variant: str) -> Path:\n",
        "    execution_dir = detect_execution_dir()\n",
        "    project_root = detect_project_root(execution_dir)\n",
        "    pm = ParamsManager(PHASE, project_root)\n",
        "    pm.set_current(variant)\n",
        "    return pm.current_variant_dir()\n",
        "\n",
        "variant_root = variant_root_from_pm(VARIANT)\n",
        "print(\"[F07] variant_root =\", variant_root)\n",
        "\n",
        "params_path = variant_root / \"params.yaml\"\n",
        "if not params_path.exists():\n",
        "    raise FileNotFoundError(f\"No existe params.yaml para {PHASE}:{VARIANT} en {params_path}\")\n",
        "\n",
        "with open(params_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    f07_params = yaml.safe_load(f)\n",
        "\n",
        "parent_f06 = f07_params.get(\"parent_variant_f06\")\n",
        "if not parent_f06:\n",
        "    raise ValueError(\"parent_variant_f06 debe estar definido en params.yaml de F07\")\n",
        "\n",
        "runtime_cfg = (f07_params.get(\"runtime\") or {})\n",
        "HOST = runtime_cfg.get(\"host\", DEFAULT_HOST)\n",
        "PORT = int(runtime_cfg.get(\"port\", DEFAULT_PORT))\n",
        "\n",
        "print(\"[F07] parent_f06 =\", parent_f06)\n",
        "print(\"[F07] runtime =\", HOST, PORT)\n",
        "\n",
        "runtime_dir = variant_root / \"runtime\"\n",
        "logs_dir = variant_root / \"logs\"\n",
        "metrics_dir = variant_root / \"metrics\"\n",
        "report_dir = variant_root / \"report\"\n",
        "figures_dir = report_dir / \"figures\"\n",
        "\n",
        "manifest_path = variant_root / \"manifest.json\"\n",
        "metadata_path = variant_root / f\"{PHASE}_metadata.json\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f9ebd1",
      "metadata": {},
      "source": [
        "## 1) Prepare - generar manifest.json\n",
        "\n",
        "`manifest.json` es el contrato sellado de F07:\n",
        "- lista de modelos (por `prediction_name`)\n",
        "- directorio del modelo en el paquete F06 (con `model.h5` y `model_summary.json`)\n",
        "- dataset asociado (parquet F04 copiado en F06)\n",
        "- columnas (`OW_events`, `label`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d899a75e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Prepare: construir manifest.json\n",
        "# ===============================\n",
        "project_root = ROOT\n",
        "f06_root = project_root / \"executions\" / \"06_packaging\" / parent_f06\n",
        "if not f06_root.exists():\n",
        "    raise FileNotFoundError(f\"No existe paquete F06: {f06_root}\")\n",
        "\n",
        "f06_metadata_path = f06_root / \"06_packaging_metadata.json\"\n",
        "if not f06_metadata_path.exists():\n",
        "    candidates = list(f06_root.glob(\"*_metadata.json\"))\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(f\"No se encontro metadata F06 en {f06_root}\")\n",
        "    f06_metadata_path = candidates[0]\n",
        "\n",
        "f06_metadata = json.loads(f06_metadata_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "models_manifest = []\n",
        "datasets_manifest = []\n",
        "\n",
        "datasets_dir_f06 = f06_root / \"datasets\"\n",
        "models_dir_f06 = f06_root / \"models\"\n",
        "\n",
        "seen_datasets = set()\n",
        "\n",
        "for m in f06_metadata.get(\"models\", []):\n",
        "    pred_name = m[\"prediction_name\"]\n",
        "    v05 = m[\"source_f05\"]\n",
        "\n",
        "    model_candidates = list(models_dir_f06.glob(f\"{pred_name}__*\"))\n",
        "    if len(model_candidates) != 1:\n",
        "        raise RuntimeError(\n",
        "            f\"Esperaba 1 directorio para modelo '{pred_name}' en {models_dir_f06}, encontrado: {model_candidates}\"\n",
        "        )\n",
        "    model_dir = model_candidates[0]\n",
        "    model_h5 = model_dir / \"model.h5\"\n",
        "    model_summary = model_dir / \"model_summary.json\"\n",
        "    if not model_h5.exists():\n",
        "        raise FileNotFoundError(f\"No existe model.h5 en {model_dir}\")\n",
        "    if not model_summary.exists():\n",
        "        raise FileNotFoundError(f\"No existe model_summary.json en {model_dir} (necesario para vectorizacion runtime)\")\n",
        "\n",
        "    f05_params_path = project_root / \"executions\" / \"05_modeling\" / v05 / \"params.yaml\"\n",
        "    if not f05_params_path.exists():\n",
        "        raise FileNotFoundError(f\"No existe params.yaml de F05 {v05}: {f05_params_path}\")\n",
        "    f05_params = yaml.safe_load(f05_params_path.read_text(encoding=\"utf-8\"))\n",
        "    v04 = f05_params[\"parent_variant\"]\n",
        "\n",
        "    dataset_path = datasets_dir_f06 / f\"{v04}__dataset.parquet\"\n",
        "    if not dataset_path.exists():\n",
        "        raise FileNotFoundError(f\"No existe dataset F04 copiado en F06: {dataset_path}\")\n",
        "\n",
        "    models_manifest.append({\n",
        "        \"prediction_name\": pred_name,\n",
        "        \"source_f05\": v05,\n",
        "        \"source_f04\": v04,\n",
        "        \"model_dir\": str(model_dir),\n",
        "        \"model_h5\": \"model.h5\",\n",
        "        \"model_summary\": \"model_summary.json\",\n",
        "        \"dataset_path\": str(dataset_path),\n",
        "        \"x_column\": \"OW_events\",\n",
        "        \"y_column\": \"label\",\n",
        "    })\n",
        "\n",
        "    if str(dataset_path) not in seen_datasets:\n",
        "        datasets_manifest.append({\n",
        "            \"dataset_path\": str(dataset_path),\n",
        "            \"x_column\": \"OW_events\",\n",
        "            \"y_column\": \"label\",\n",
        "            \"source_f04\": v04,\n",
        "        })\n",
        "        seen_datasets.add(str(dataset_path))\n",
        "\n",
        "manifest = {\n",
        "    \"phase\": PHASE,\n",
        "    \"variant\": VARIANT,\n",
        "    \"f06_variant\": parent_f06,\n",
        "    \"f06_path\": str(f06_root),\n",
        "    \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"runtime\": {\"host\": HOST, \"port\": PORT},\n",
        "    \"models\": models_manifest,\n",
        "    \"datasets\": datasets_manifest,\n",
        "}\n",
        "\n",
        "manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
        "print(\"[OK] manifest.json generado en:\", manifest_path)\n",
        "print(\"[OK] modelos:\", len(models_manifest), \"| datasets:\", len(datasets_manifest))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0f552c",
      "metadata": {},
      "source": [
        "## 2) Run - servidor + cliente (batch) + logs crudos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14eb444",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Servidor Flask en thread (para notebook)\n",
        "# ===============================\n",
        "class ServerThread(Thread):\n",
        "    def __init__(self, app, host, port):\n",
        "        super().__init__(daemon=True)\n",
        "        self.server = make_server(host, port, app)\n",
        "        self.ctx = app.app_context()\n",
        "        self.ctx.push()\n",
        "\n",
        "    def run(self):\n",
        "        self.server.serve_forever()\n",
        "\n",
        "    def shutdown(self):\n",
        "        self.server.shutdown()\n",
        "\n",
        "\n",
        "def build_runtime_server(manifest: dict) -> Flask:\n",
        "    app = Flask(__name__)\n",
        "\n",
        "    loaded_models = []\n",
        "    for m in manifest[\"models\"]:\n",
        "        model_dir = Path(m[\"model_dir\"])\n",
        "        summary = json.loads((model_dir / m[\"model_summary\"]).read_text(encoding=\"utf-8\"))\n",
        "        model = tf.keras.models.load_model(model_dir / m[\"model_h5\"])\n",
        "\n",
        "        loaded_models.append({\n",
        "            \"prediction_name\": summary[\"prediction_name\"],\n",
        "            \"model\": model,\n",
        "            \"vectorization\": summary[\"vectorization\"],\n",
        "            \"threshold\": float(summary.get(\"threshold\", 0.5)),\n",
        "        })\n",
        "\n",
        "    def vectorize_dense_bow(window, cfg):\n",
        "        vocab = cfg[\"vocab\"]\n",
        "        input_dim = int(cfg[\"input_dim\"])\n",
        "        index = {int(ev): i for i, ev in enumerate(vocab)}\n",
        "        X = np.zeros((1, input_dim), dtype=np.float32)\n",
        "        for ev in window:\n",
        "            i = index.get(int(ev))\n",
        "            if i is not None:\n",
        "                X[0, i] += 1.0\n",
        "        return X\n",
        "\n",
        "    def vectorize_sequence(window, cfg):\n",
        "        vocab = cfg[\"vocab\"]\n",
        "        max_len = int(cfg[\"max_len\"])\n",
        "        index = {int(ev): i + 1 for i, ev in enumerate(vocab)}\n",
        "        seq = [index[int(e)] for e in window if int(e) in index]\n",
        "        seq = seq[-max_len:]\n",
        "        X = np.zeros((1, max_len), dtype=np.int32)\n",
        "        if len(seq) > 0:\n",
        "            X[0, -len(seq):] = np.array(seq, dtype=np.int32)\n",
        "        return X\n",
        "\n",
        "    def vectorize(window, cfg):\n",
        "        vtype = cfg.get(\"vectorization\")\n",
        "        if vtype == \"dense_bow\":\n",
        "            return vectorize_dense_bow(window, cfg)\n",
        "        if vtype == \"sequence\":\n",
        "            return vectorize_sequence(window, cfg)\n",
        "        raise ValueError(f\"Vectorization no soportada: {vtype}\")\n",
        "\n",
        "    @app.route(\"/infer\", methods=[\"POST\"])\n",
        "    def infer():\n",
        "        payload = request.get_json(force=True)\n",
        "        window = payload[\"window\"]\n",
        "        results = []\n",
        "        for m in loaded_models:\n",
        "            X = vectorize(window, m[\"vectorization\"])\n",
        "            y_prob = float(m[\"model\"].predict(X, verbose=0).ravel()[0])\n",
        "            y_pred = int(y_prob >= m[\"threshold\"])\n",
        "            results.append({\"prediction_name\": m[\"prediction_name\"], \"y_pred\": y_pred})\n",
        "        return jsonify({\"window\": window, \"results\": results})\n",
        "\n",
        "    @app.route(\"/control\", methods=[\"POST\"])\n",
        "    def control():\n",
        "        payload = request.get_json(force=True)\n",
        "        if payload.get(\"cmd\") == \"shutdown\":\n",
        "            return jsonify({\"status\": \"shutting_down\"})\n",
        "        return jsonify({\"status\": \"unknown_command\"})\n",
        "\n",
        "    return app\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d636e487",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Run: orquestacion idempotente\n",
        "# ===============================\n",
        "manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "ensure_clean_dir(runtime_dir)\n",
        "ensure_clean_dir(logs_dir)\n",
        "ensure_clean_dir(metrics_dir)\n",
        "ensure_clean_dir(report_dir)\n",
        "figures_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def to_json_safe_window(window):\n",
        "    if hasattr(window, \"tolist\"):\n",
        "        return window.tolist()\n",
        "    if isinstance(window, (list, tuple)):\n",
        "        return list(window)\n",
        "    if hasattr(window, \"item\"):\n",
        "        return [window.item()]\n",
        "    return [window]\n",
        "\n",
        "app = build_runtime_server(manifest)\n",
        "server_thread = ServerThread(app, HOST, PORT)\n",
        "server_thread.start()\n",
        "\n",
        "base_url = f\"http://{HOST}:{PORT}\"\n",
        "for _ in range(50):\n",
        "    try:\n",
        "        r = requests.post(f\"{base_url}/infer\", json={\"window\": []}, timeout=2)\n",
        "        if r.status_code == 200:\n",
        "            break\n",
        "    except Exception:\n",
        "        time.sleep(0.1)\n",
        "else:\n",
        "    server_thread.shutdown()\n",
        "    raise RuntimeError(\"El servidor no ha arrancado correctamente\")\n",
        "\n",
        "raw_rows = []\n",
        "for dataset_idx, ds in enumerate(manifest[\"datasets\"], start=1):\n",
        "    df = pd.read_parquet(ds[\"dataset_path\"])\n",
        "    xcol = ds[\"x_column\"]\n",
        "    dataset_name = Path(ds[\"dataset_path\"]).name\n",
        "    total_rows = len(df)\n",
        "\n",
        "    print(\n",
        "        f\"[INFO] Dataset {dataset_idx}/{len(manifest['datasets'])} {dataset_name}: \"\n",
        "        f\"procesando {total_rows} ventanas\"\n",
        "    )\n",
        "\n",
        "    progress_every = max(1000, total_rows // 20) if total_rows > 0 else 1000\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        window_payload = to_json_safe_window(row[xcol])\n",
        "\n",
        "        resp = requests.post(f\"{base_url}/infer\", json={\"window\": window_payload}, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "\n",
        "        window_str = json.dumps(data[\"window\"], separators=(\",\", \":\"), ensure_ascii=False)\n",
        "        for rr in data[\"results\"]:\n",
        "            raw_rows.append({\n",
        "                \"window\": window_str,\n",
        "                \"prediction_name\": rr[\"prediction_name\"],\n",
        "                \"y_pred\": int(rr[\"y_pred\"]),\n",
        "            })\n",
        "\n",
        "        processed = idx + 1\n",
        "        if processed % progress_every == 0 or processed == total_rows:\n",
        "            print(\n",
        "                f\"[RUN] {dataset_name}: enviadas {processed}/{total_rows} \"\n",
        "                f\"ventanas ({(processed / total_rows * 100):.1f}%)\"\n",
        "            )\n",
        "\n",
        "raw_df = pd.DataFrame(raw_rows)\n",
        "raw_parquet_path = logs_dir / \"raw_predictions.parquet\"\n",
        "raw_csv_path = logs_dir / \"raw_predictions.csv\"\n",
        "raw_df.to_parquet(raw_parquet_path, index=False)\n",
        "raw_df.to_csv(raw_csv_path, index=False)\n",
        "\n",
        "try:\n",
        "    requests.post(f\"{base_url}/control\", json={\"cmd\": \"shutdown\"}, timeout=5)\n",
        "finally:\n",
        "    server_thread.shutdown()\n",
        "\n",
        "print(\"[OK] logs crudos guardados:\")\n",
        "print(\" -\", raw_parquet_path)\n",
        "print(\" -\", raw_csv_path)\n",
        "print(\"[OK] filas:\", len(raw_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19450b89",
      "metadata": {},
      "source": [
        "## 3) Postprocess - metricas por modelo (`prediction_name`) + `no_ref_*`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4aaa5dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
        "raw_df = pd.read_parquet(raw_parquet_path)\n",
        "\n",
        "# Indice rapido: (prediction_name, window_str) -> y_pred (primera aparicion)\n",
        "pred_map = {}\n",
        "for row in raw_df.itertuples(index=False):\n",
        "    key = (row.prediction_name, row.window)\n",
        "    if key not in pred_map:\n",
        "        pred_map[key] = int(row.y_pred)\n",
        "\n",
        "metrics_rows = []\n",
        "\n",
        "def to_json_safe_window(window):\n",
        "    if hasattr(window, \"tolist\"):\n",
        "        return window.tolist()\n",
        "    if isinstance(window, (list, tuple)):\n",
        "        return list(window)\n",
        "    if hasattr(window, \"item\"):\n",
        "        return [window.item()]\n",
        "    return [window]\n",
        "\n",
        "for model_idx, m in enumerate(manifest[\"models\"], start=1):\n",
        "    pred_name = m[\"prediction_name\"]\n",
        "    dataset_path = Path(m[\"dataset_path\"])\n",
        "    xcol = m[\"x_column\"]\n",
        "    ycol = m[\"y_column\"]\n",
        "\n",
        "    df = pd.read_parquet(dataset_path)\n",
        "    total_rows = len(df)\n",
        "    print(\n",
        "        f\"[INFO] MÃ©tricas {model_idx}/{len(manifest['models'])} para {pred_name} \"\n",
        "        f\"sobre {total_rows} ventanas\"\n",
        "    )\n",
        "    progress_every = max(1000, total_rows // 20) if total_rows > 0 else 1000\n",
        "\n",
        "    ref_windows = set()\n",
        "    tp = tn = fp = fn = 0\n",
        "\n",
        "    for idx, row in enumerate(df.itertuples(index=False), start=1):\n",
        "        window = getattr(row, xcol)\n",
        "        y_true = int(getattr(row, ycol))\n",
        "        window_list = to_json_safe_window(window)\n",
        "        window_str = json.dumps(window_list, separators=(\",\", \":\"), ensure_ascii=False)\n",
        "        ref_windows.add(window_str)\n",
        "\n",
        "        y_pred = pred_map.get((pred_name, window_str))\n",
        "        if y_pred is None:\n",
        "            continue\n",
        "\n",
        "        if y_true == 1 and y_pred == 1:\n",
        "            tp += 1\n",
        "        elif y_true == 0 and y_pred == 0:\n",
        "            tn += 1\n",
        "        elif y_true == 0 and y_pred == 1:\n",
        "            fp += 1\n",
        "        elif y_true == 1 and y_pred == 0:\n",
        "            fn += 1\n",
        "\n",
        "        if idx % progress_every == 0 or idx == total_rows:\n",
        "            print(\n",
        "                f\"[METRICS] {pred_name}: procesadas {idx}/{total_rows} \"\n",
        "                f\"ventanas ({(idx / total_rows * 100):.1f}%)\"\n",
        "            )\n",
        "\n",
        "    model_preds = raw_df[raw_df[\"prediction_name\"] == pred_name]\n",
        "    no_ref_pred_1 = int(((~model_preds[\"window\"].isin(ref_windows)) & (model_preds[\"y_pred\"] == 1)).sum())\n",
        "    no_ref_pred_0 = int(((~model_preds[\"window\"].isin(ref_windows)) & (model_preds[\"y_pred\"] == 0)).sum())\n",
        "    no_ref_total = int(no_ref_pred_0 + no_ref_pred_1)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
        "\n",
        "    metrics_rows.append({\n",
        "        \"prediction_name\": pred_name,\n",
        "        \"source_f05\": m[\"source_f05\"],\n",
        "        \"source_f04\": m[\"source_f04\"],\n",
        "        \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"no_ref_pred_1\": no_ref_pred_1,\n",
        "        \"no_ref_pred_0\": no_ref_pred_0,\n",
        "        \"no_ref_total\": no_ref_total,\n",
        "    })\n",
        "\n",
        "    cm = np.array([[tn, fp], [fn, tp]], dtype=int)\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(cm)\n",
        "    plt.title(f\"Confusion - {pred_name}\")\n",
        "    plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n",
        "\n",
        "    plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n",
        "    for (i, j), v in np.ndenumerate(cm):\n",
        "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / f\"confusion_{pred_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "metrics_csv_path = metrics_dir / \"metrics_per_model.csv\"\n",
        "metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "\n",
        "print(\"[OK] metricas guardadas:\", metrics_csv_path)\n",
        "metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f25d537",
      "metadata": {},
      "source": [
        "## 4) Report - HTML + figuras (`report/figures/`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd51503",
      "metadata": {},
      "outputs": [],
      "source": [
        "def html_escape(s: str) -> str:\n",
        "    return (\n",
        "        s.replace(\"&\", \"&amp;\")\n",
        "         .replace(\"<\", \"&lt;\")\n",
        "         .replace(\">\", \"&gt;\")\n",
        "         .replace('\"', \"&quot;\")\n",
        "         .replace(\"'\", \"&#39;\")\n",
        "    )\n",
        "\n",
        "rows_html = metrics_df.to_html(index=False)\n",
        "\n",
        "imgs = []\n",
        "for m in manifest[\"models\"]:\n",
        "    pred_name = m[\"prediction_name\"]\n",
        "    img_rel = f\"figures/confusion_{pred_name}.png\"\n",
        "    img_path = figures_dir / f\"confusion_{pred_name}.png\"\n",
        "    if img_path.exists():\n",
        "        imgs.append(f\"<h3>{html_escape(pred_name)}</h3><img src='{img_rel}' style='max-width:420px;'/>\")\n",
        "\n",
        "report_html = f\"\"\"<!doctype html>\n",
        "<html>\n",
        "<head>\n",
        "  <meta charset=\"utf-8\"/>\n",
        "  <title>F07 Report - {html_escape(VARIANT)}</title>\n",
        "  <style>\n",
        "    body {{ font-family: Arial, sans-serif; margin: 24px; }}\n",
        "    table {{ border-collapse: collapse; }}\n",
        "    th, td {{ border: 1px solid #ddd; padding: 6px 10px; }}\n",
        "    th {{ background: #f3f3f3; }}\n",
        "    code {{ background:#f7f7f7; padding:2px 4px; }}\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <h1>F07 - Deploy & Runtime Validation</h1>\n",
        "  <p><b>Variant:</b> <code>{html_escape(VARIANT)}</code></p>\n",
        "  <p><b>Parent F06:</b> <code>{html_escape(parent_f06)}</code></p>\n",
        "  <p><b>Generated:</b> {datetime.now(timezone.utc).isoformat()}</p>\n",
        "\n",
        "  <h2>Metrics per model</h2>\n",
        "  {rows_html}\n",
        "\n",
        "  <h2>Confusion matrices</h2>\n",
        "  {''.join(imgs)}\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "report_path = report_dir / \"report.html\"\n",
        "report_path.write_text(report_html, encoding=\"utf-8\")\n",
        "print(\"[OK] report:\", report_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f925fa61",
      "metadata": {},
      "source": [
        "## 5) Trazabilidad - `07_deployrun_metadata.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5edce9",
      "metadata": {},
      "outputs": [],
      "source": [
        "write_metadata(\n",
        "    stage=PHASE,\n",
        "    variant=VARIANT,\n",
        "    parent_variant=parent_f06,\n",
        "    inputs=[str(manifest_path)],\n",
        "    outputs=[str(logs_dir), str(metrics_dir), str(report_dir)],\n",
        "    params=f07_params,\n",
        "    metadata_path=metadata_path,\n",
        ")\n",
        "\n",
        "print(\"[OK] metadata:\", metadata_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cabccf",
      "metadata": {},
      "source": [
        "## 6) Resumen de artefactos generados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d30f0b67",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"== Artefactos F07 ==\")\n",
        "print(\"manifest :\", manifest_path)\n",
        "print(\"logs     :\", raw_parquet_path, raw_csv_path)\n",
        "print(\"metrics  :\", metrics_csv_path)\n",
        "print(\"report   :\", report_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
