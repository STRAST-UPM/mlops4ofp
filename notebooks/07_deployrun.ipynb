{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04fa8a62",
   "metadata": {},
   "source": [
    "# Fase 07 — Deploy & Runtime Validation (Notebook)\n",
    " \n",
    "Este notebook ejecuta la Fase 07:\n",
    "\n",
    "- Arranca un servidor Flask de inferencia.\n",
    "- Envía **solo ventanas de observación únicas** (optimización fuerte).\n",
    "- Recoge predicciones de todos los modelos del paquete F06.\n",
    "- Calcula métricas por modelo (TP, TN, FP, FN, precision, recall, f1).\n",
    "- Genera un informe HTML simple y figuras de matriz de confusión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51557cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:26:49.929129Z",
     "iopub.status.busy": "2026-02-19T17:26:49.928946Z",
     "iopub.status.idle": "2026-02-19T17:26:50.063708Z",
     "shell.execute_reply": "2026-02-19T17:26:50.061775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PHASE = \"07_deployrun\"\n",
    "VARIANT = os.environ.get(\"ACTIVE_VARIANT\", \"v701\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from time import perf_counter\n",
    "import shutil\n",
    "import yaml\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "SCRIPT_PATH = Path.cwd().resolve()\n",
    "ROOT = SCRIPT_PATH\n",
    "for _ in range(10):\n",
    "    if (ROOT / \"mlops4ofp\").exists():\n",
    "        break\n",
    "    ROOT = ROOT.parent\n",
    "else:\n",
    "    raise RuntimeError(\"No se pudo localizar project root\")\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print(\"Project root:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b8b402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:26:50.068138Z",
     "iopub.status.busy": "2026-02-19T17:26:50.067899Z",
     "iopub.status.idle": "2026-02-19T17:26:53.792660Z",
     "shell.execute_reply": "2026-02-19T17:26:53.792247Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from werkzeug.serving import make_server\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "\n",
    "from mlops4ofp.tools.params_manager import ParamsManager\n",
    "from mlops4ofp.tools.run_context import detect_execution_dir, detect_project_root\n",
    "from mlops4ofp.tools.traceability import write_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8d5089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:26:53.794749Z",
     "iopub.status.busy": "2026-02-19T17:26:53.794491Z",
     "iopub.status.idle": "2026-02-19T17:26:53.802448Z",
     "shell.execute_reply": "2026-02-19T17:26:53.802141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTX] Fase: 07_deployrun  Variante: v701\n",
      "[CTX] execution_dir = /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/notebooks\n",
      "[CTX] project_root   = /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp\n",
      "[CTX] variant_root   = /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701\n",
      "[CFG] parent_f06             = v601\n",
      "[CFG] batch_size             = 512\n",
      "[CFG] sample_size            = None\n",
      "[CFG] infer_batch_retries    = 3\n",
      "[CFG] progress_every_batches = 10\n",
      "[MANIFEST] F06: v601\n",
      "[MANIFEST] modelos empaquetados: 4\n",
      "[MANIFEST] datasets F04: 4\n"
     ]
    }
   ],
   "source": [
    "PHASE = \"07_deployrun\"\n",
    "\n",
    "# Variante activa: leída del entorno (make nb7-run establece ACTIVE_VARIANT)\n",
    "VARIANT = os.environ.get(\"ACTIVE_VARIANT\", None)\n",
    "if not VARIANT:\n",
    "    raise RuntimeError(\"ACTIVE_VARIANT no definido. Ejecuta con make nb7-run VARIANT=v7XX\")\n",
    "\n",
    "print(f\"[CTX] Fase: {PHASE}  Variante: {VARIANT}\")\n",
    "\n",
    "# Detectar contexto de ejecución\n",
    "execution_dir = detect_execution_dir()\n",
    "project_root = detect_project_root(execution_dir)\n",
    "\n",
    "print(f\"[CTX] execution_dir = {execution_dir}\")\n",
    "print(f\"[CTX] project_root   = {project_root}\")\n",
    "\n",
    "pm = ParamsManager(PHASE, project_root)\n",
    "pm.set_current(VARIANT)\n",
    "variant_root = pm.current_variant_dir()\n",
    "\n",
    "print(f\"[CTX] variant_root   = {variant_root}\")\n",
    "\n",
    "# Cargar params.yaml\n",
    "params_path = variant_root / \"params.yaml\"\n",
    "if not params_path.exists():\n",
    "    raise FileNotFoundError(f\"No existe params.yaml en {params_path}\")\n",
    "\n",
    "with open(params_path, \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "parent_f06 = params[\"parent_variant_f06\"]\n",
    "batch_size = params.get(\"batch_size\", 256)\n",
    "sample_size = params.get(\"sample_size\", None)\n",
    "infer_batch_retries = params.get(\"infer_batch_retries\", 3)\n",
    "progress_every_batches = params.get(\"progress_every_batches\", 10)\n",
    "\n",
    "print(f\"[CFG] parent_f06             = {parent_f06}\")\n",
    "print(f\"[CFG] batch_size             = {batch_size}\")\n",
    "print(f\"[CFG] sample_size            = {sample_size}\")\n",
    "print(f\"[CFG] infer_batch_retries    = {infer_batch_retries}\")\n",
    "print(f\"[CFG] progress_every_batches = {progress_every_batches}\")\n",
    "\n",
    "# Directorios de artefactos\n",
    "runtime_dir = variant_root / \"runtime\"\n",
    "logs_dir = variant_root / \"logs\"\n",
    "metrics_dir = variant_root / \"metrics\"\n",
    "report_dir = variant_root / \"report\"\n",
    "figures_dir = report_dir / \"figures\"\n",
    "\n",
    "for d in [runtime_dir, logs_dir, metrics_dir, report_dir, figures_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Manifest\n",
    "manifest_path = variant_root / \"manifest.json\"\n",
    "if not manifest_path.exists():\n",
    "    raise FileNotFoundError(f\"manifest.json no encontrado en {manifest_path}. Ejecuta make variant7 antes.\")\n",
    "\n",
    "with open(manifest_path, \"r\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "print(f\"[MANIFEST] F06: {manifest.get('f06_variant', parent_f06)}\")\n",
    "print(f\"[MANIFEST] modelos empaquetados: {len(manifest['models'])}\")\n",
    "print(f\"[MANIFEST] datasets F04: {len(manifest['datasets'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d395cd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:26:53.803862Z",
     "iopub.status.busy": "2026-02-19T17:26:53.803776Z",
     "iopub.status.idle": "2026-02-19T17:26:53.807598Z",
     "shell.execute_reply": "2026-02-19T17:26:53.807344Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_json_safe_window(window):\n",
    "    \"\"\"Normaliza la ventana OW_events a lista nativa para poder serializarla.\"\"\"\n",
    "    if hasattr(window, \"tolist\"):\n",
    "        return window.tolist()\n",
    "    if isinstance(window, (list, tuple)):\n",
    "        return list(window)\n",
    "    if hasattr(window, \"item\"):\n",
    "        return [window.item()]\n",
    "    return [window]\n",
    "\n",
    "# Paths para logs crudos\n",
    "raw_path_parquet = logs_dir / \"raw_predictions.parquet\"\n",
    "raw_path_csv = logs_dir / \"raw_predictions.csv\"\n",
    "\n",
    "def write_batch_predictions(batch_df: pd.DataFrame):\n",
    "    \"\"\"Append incremental de logs crudos en CSV y Parquet.\n",
    "\n",
    "    Dado que el número de ventanas únicas es relativamente pequeño,\n",
    "    podemos permitirnos read+concat+write para Parquet.\n",
    "    \"\"\"\n",
    "    # CSV (append)\n",
    "    if not raw_path_csv.exists():\n",
    "        batch_df.to_csv(raw_path_csv, index=False)\n",
    "    else:\n",
    "        batch_df.to_csv(raw_path_csv, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    # Parquet (read+concat+overwrite; coste asumible por tamaño reducido)\n",
    "    if not raw_path_parquet.exists():\n",
    "        batch_df.to_parquet(raw_path_parquet, index=False)\n",
    "    else:\n",
    "        existing = pd.read_parquet(raw_path_parquet)\n",
    "        combined = pd.concat([existing, batch_df], ignore_index=True)\n",
    "        combined.to_parquet(raw_path_parquet, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8c7d6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:26:53.808896Z",
     "iopub.status.busy": "2026-02-19T17:26:53.808818Z",
     "iopub.status.idle": "2026-02-19T17:26:54.014152Z",
     "shell.execute_reply": "2026-02-19T17:26:54.013855Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [19/Feb/2026 18:26:54] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SERVER] Flask listo: {'models': 4, 'status': 'ready'}\n"
     ]
    }
   ],
   "source": [
    "HOST = \"127.0.0.1\"\n",
    "PORT = 5005\n",
    "base_url = f\"http://{HOST}:{PORT}\"\n",
    "\n",
    "def build_flask_app(manifest: dict) -> Flask:\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    loaded_models = []\n",
    "\n",
    "    for m in manifest[\"models\"]:\n",
    "        model_dir = Path(m[\"model_dir\"])\n",
    "        summary_path = model_dir / m[\"model_summary\"]\n",
    "        model_path = model_dir / m[\"model_h5\"]\n",
    "\n",
    "        summary = json.loads(summary_path.read_text())\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        loaded_models.append({\n",
    "            \"prediction_name\": summary[\"prediction_name\"],\n",
    "            \"model\": model,\n",
    "            \"vectorization\": summary[\"vectorization\"],\n",
    "            \"threshold\": summary.get(\"threshold\", 0.5),\n",
    "        })\n",
    "\n",
    "    def vectorize_batch(windows, cfg):\n",
    "        if cfg[\"vectorization\"] == \"dense_bow\":\n",
    "            vocab = cfg[\"vocab\"]\n",
    "            index = {ev: i for i, ev in enumerate(vocab)}\n",
    "            X = np.zeros((len(windows), cfg[\"input_dim\"]), dtype=np.float32)\n",
    "            for i, window in enumerate(windows):\n",
    "                for ev in window:\n",
    "                    if ev in index:\n",
    "                        X[i, index[ev]] += 1.0\n",
    "            return X\n",
    "\n",
    "        if cfg[\"vectorization\"] == \"sequence\":\n",
    "            vocab = cfg[\"vocab\"]\n",
    "            index = {ev: i + 1 for i, ev in enumerate(vocab)}\n",
    "            max_len = cfg[\"max_len\"]\n",
    "            X = np.zeros((len(windows), max_len), dtype=np.int32)\n",
    "            for i, window in enumerate(windows):\n",
    "                seq = [index[e] for e in window if e in index]\n",
    "                seq = seq[-max_len:]\n",
    "                if len(seq) > 0:\n",
    "                    X[i, -len(seq):] = seq\n",
    "            return X\n",
    "\n",
    "        raise ValueError(\"vectorization no soportada: \" + str(cfg.get(\"vectorization\")))\n",
    "\n",
    "    @app.route(\"/\", methods=[\"GET\"])\n",
    "    def health():\n",
    "        return jsonify({\"status\": \"ready\", \"models\": len(loaded_models)})\n",
    "\n",
    "    @app.route(\"/infer_batch\", methods=[\"POST\"])\n",
    "    def infer_batch():\n",
    "        payload = request.get_json(force=True)\n",
    "        windows = payload[\"windows\"]\n",
    "\n",
    "        batch_results = []\n",
    "\n",
    "        for m in loaded_models:\n",
    "            X_batch = vectorize_batch(windows, m[\"vectorization\"])\n",
    "            y_probs = m[\"model\"].predict(X_batch, verbose=0).flatten()\n",
    "            y_preds = (y_probs >= m[\"threshold\"]).astype(int)\n",
    "\n",
    "            for i, window in enumerate(windows):\n",
    "                if len(batch_results) <= i:\n",
    "                    batch_results.append({\"window\": window, \"results\": []})\n",
    "\n",
    "                batch_results[i][\"results\"].append({\n",
    "                    \"prediction_name\": m[\"prediction_name\"],\n",
    "                    \"y_pred\": int(y_preds[i]),\n",
    "                })\n",
    "\n",
    "        return jsonify({\"results\": batch_results})\n",
    "\n",
    "    @app.route(\"/control\", methods=[\"POST\"])\n",
    "    def control():\n",
    "        payload = request.get_json(force=True) or {}\n",
    "        if payload.get(\"cmd\") == \"shutdown\":\n",
    "            func = request.environ.get(\"werkzeug.server.shutdown\")\n",
    "            if func:\n",
    "                func()\n",
    "            return jsonify({\"status\": \"shutting_down\"})\n",
    "        return jsonify({\"status\": \"unknown_command\"})\n",
    "\n",
    "    @app.errorhandler(Exception)\n",
    "    def handle_exception(err):\n",
    "        tb = traceback.format_exc()\n",
    "        return jsonify({\"error\": str(err), \"traceback\": tb}), 500\n",
    "\n",
    "    return app\n",
    "\n",
    "class ServerThread:\n",
    "    def __init__(self, app: Flask, host: str, port: int):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self._server = make_server(host, port, app)\n",
    "        self._ctx = app.app_context()\n",
    "        self._ctx.push()\n",
    "\n",
    "    def start(self):\n",
    "        import threading\n",
    "        self._thread = threading.Thread(target=self._server.serve_forever, daemon=True)\n",
    "        self._thread.start()\n",
    "\n",
    "    def shutdown(self):\n",
    "        self._server.shutdown()\n",
    "        if hasattr(self, \"_thread\"):\n",
    "            self._thread.join(timeout=2.0)\n",
    "\n",
    "# Arrancar servidor\n",
    "app = build_flask_app(manifest)\n",
    "server_thread = ServerThread(app, HOST, PORT)\n",
    "server_thread.start()\n",
    "\n",
    "# Esperar a que responda\n",
    "for attempt in range(10):\n",
    "    try:\n",
    "        r = requests.get(base_url + \"/\", timeout=0.5)\n",
    "        if r.status_code == 200:\n",
    "            print(\"[SERVER] Flask listo:\", r.json())\n",
    "            break\n",
    "    except Exception:\n",
    "        time.sleep(0.5)\n",
    "else:\n",
    "    raise RuntimeError(\"Servidor Flask no respondió al healthcheck.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cf6b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:26:54.015785Z",
     "iopub.status.busy": "2026-02-19T17:26:54.015658Z",
     "iopub.status.idle": "2026-02-19T17:26:54.019362Z",
     "shell.execute_reply": "2026-02-19T17:26:54.019081Z"
    }
   },
   "outputs": [],
   "source": [
    "def post_infer_batch_with_retry(windows, dataset_name, processed_count, total_count):\n",
    "    last_error = None\n",
    "\n",
    "    for attempt in range(1, infer_batch_retries + 1):\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{base_url}/infer_batch\",\n",
    "                json={\"windows\": windows},\n",
    "                timeout=120,\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            if attempt > 1:\n",
    "                print(\n",
    "                    f\"[INFO] infer_batch recuperado tras {attempt} intentos \"\n",
    "                    f\"en {dataset_name} ({processed_count}/{total_count})\",\n",
    "                    flush=True,\n",
    "                )\n",
    "            return resp.json()\n",
    "        except requests.RequestException as err:\n",
    "            last_error = err\n",
    "            print(\n",
    "                f\"[WARN] infer_batch fallo intento {attempt}/{infer_batch_retries} \"\n",
    "                f\"en {dataset_name} ({processed_count}/{total_count}): {err}\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "            if attempt == infer_batch_retries:\n",
    "                raise RuntimeError(\n",
    "                    \"infer_batch agotó reintentos. \"\n",
    "                    f\"Dataset={dataset_name}, progreso={processed_count}/{total_count}, \"\n",
    "                    f\"último_error={last_error}\"\n",
    "                ) from err\n",
    "\n",
    "            backoff_seconds = min(2 ** (attempt - 1), 5)\n",
    "            time.sleep(backoff_seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce89b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:26:54.020675Z",
     "iopub.status.busy": "2026-02-19T17:26:54.020594Z",
     "iopub.status.idle": "2026-02-19T17:36:04.111469Z",
     "shell.execute_reply": "2026-02-19T17:36:04.111157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Dataset base: v401__dataset.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Filas totales: 2895405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Serializando ventanas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ventanas únicas detectadas: 310521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reducción: 2895405 → 310521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] batch_size=512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Iniciando inferencia sobre ventanas únicas...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 5120/310521 (1.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 10240/310521 (3.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 15360/310521 (4.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 20480/310521 (6.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 25600/310521 (8.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 30720/310521 (9.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 35840/310521 (11.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 40960/310521 (13.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 46080/310521 (14.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 51200/310521 (16.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 56320/310521 (18.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 61440/310521 (19.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 66560/310521 (21.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 71680/310521 (23.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 76800/310521 (24.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 81920/310521 (26.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 87040/310521 (28.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 92160/310521 (29.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 97280/310521 (31.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 102400/310521 (33.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 107520/310521 (34.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 112640/310521 (36.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 117760/310521 (37.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 122880/310521 (39.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 128000/310521 (41.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 133120/310521 (42.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 138240/310521 (44.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 143360/310521 (46.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 148480/310521 (47.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 153600/310521 (49.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 158720/310521 (51.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 163840/310521 (52.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 168960/310521 (54.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 174080/310521 (56.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 179200/310521 (57.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 184320/310521 (59.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 189440/310521 (61.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 194560/310521 (62.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 199680/310521 (64.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 204800/310521 (66.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 209920/310521 (67.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 215040/310521 (69.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 220160/310521 (70.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 225280/310521 (72.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 230400/310521 (74.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 235520/310521 (75.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 240640/310521 (77.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 245760/310521 (79.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 250880/310521 (80.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 256000/310521 (82.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 261120/310521 (84.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 266240/310521 (85.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 271360/310521 (87.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 276480/310521 (89.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 281600/310521 (90.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 286720/310521 (92.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 291840/310521 (94.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 296960/310521 (95.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 302080/310521 (97.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 307200/310521 (98.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Ventanas únicas procesadas 310521/310521 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Inferencia completada.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] logs crudos guardados:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/logs/raw_predictions.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/logs/raw_predictions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] filas únicas inferidas: 1242084\n"
     ]
    }
   ],
   "source": [
    "raw_df = None\n",
    "\n",
    "try:\n",
    "    # =============================\n",
    "    # 1) Dataset base (único)\n",
    "    # =============================\n",
    "    base_dataset = manifest[\"datasets\"][0]\n",
    "    df = pd.read_parquet(base_dataset[\"dataset_path\"])\n",
    "\n",
    "    if sample_size:\n",
    "        df = df.head(sample_size)\n",
    "\n",
    "    x_column = base_dataset[\"x_column\"]\n",
    "    dataset_name = Path(base_dataset[\"dataset_path\"]).name\n",
    "\n",
    "    print(f\"\\n[INFO] Dataset base: {dataset_name}\", flush=True)\n",
    "    print(f\"[INFO] Filas totales: {len(df)}\", flush=True)\n",
    "\n",
    "    # =============================\n",
    "    # 2) Serializar y deduplicar ventanas\n",
    "    # =============================\n",
    "    print(\"[INFO] Serializando ventanas...\", flush=True)\n",
    "\n",
    "    df[\"window\"] = df[x_column].apply(\n",
    "        lambda w: json.dumps(\n",
    "            to_json_safe_window(w),\n",
    "            separators=(\",\", \":\"),\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    unique_windows = df[\"window\"].unique()\n",
    "    total_unique = len(unique_windows)\n",
    "\n",
    "    print(f\"[INFO] Ventanas únicas detectadas: {total_unique}\", flush=True)\n",
    "    print(f\"[INFO] Reducción: {len(df)} → {total_unique}\", flush=True)\n",
    "    print(f\"[INFO] batch_size={batch_size}\", flush=True)\n",
    "\n",
    "    # =============================\n",
    "    # 3) Inferencia sobre ventanas únicas\n",
    "    # =============================\n",
    "    processed_unique = 0\n",
    "    progress_stride = max(batch_size * progress_every_batches, 1)\n",
    "\n",
    "    print(\"\\n[INFO] Iniciando inferencia sobre ventanas únicas...\\n\", flush=True)\n",
    "\n",
    "    # Limpiar logs previos si existían\n",
    "    if raw_path_parquet.exists():\n",
    "        raw_path_parquet.unlink()\n",
    "    if raw_path_csv.exists():\n",
    "        raw_path_csv.unlink()\n",
    "\n",
    "    for i in range(0, total_unique, batch_size):\n",
    "        batch_window_json = unique_windows[i:i + batch_size]\n",
    "\n",
    "        batch_windows = [json.loads(w) for w in batch_window_json]\n",
    "\n",
    "        data = post_infer_batch_with_retry(\n",
    "            windows=batch_windows,\n",
    "            dataset_name=dataset_name,\n",
    "            processed_count=min(i + batch_size, total_unique),\n",
    "            total_count=total_unique,\n",
    "        )\n",
    "\n",
    "        batch_rows = []\n",
    "        for item in data[\"results\"]:\n",
    "            window_json = json.dumps(\n",
    "                item[\"window\"], separators=(\",\", \":\"), ensure_ascii=False\n",
    "            )\n",
    "            for r in item[\"results\"]:\n",
    "                batch_rows.append({\n",
    "                    \"window\": window_json,\n",
    "                    \"prediction_name\": r[\"prediction_name\"],\n",
    "                    \"y_pred\": r[\"y_pred\"],\n",
    "                })\n",
    "\n",
    "        batch_df = pd.DataFrame(batch_rows)\n",
    "        write_batch_predictions(batch_df)\n",
    "\n",
    "        processed_unique += len(batch_windows)\n",
    "\n",
    "        if processed_unique % progress_stride == 0 or processed_unique == total_unique:\n",
    "            pct = processed_unique / total_unique * 100\n",
    "            print(\n",
    "                f\"[RUN] Ventanas únicas procesadas \"\n",
    "                f\"{processed_unique}/{total_unique} ({pct:.1f}%)\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "    print(\"\\n[INFO] Inferencia completada.\", flush=True)\n",
    "\n",
    "    # =============================\n",
    "    # 4) Cargar logs crudos\n",
    "    # =============================\n",
    "    if raw_path_parquet.exists():\n",
    "        raw_df = pd.read_parquet(raw_path_parquet)\n",
    "    else:\n",
    "        raw_df = pd.DataFrame(columns=[\"window\", \"prediction_name\", \"y_pred\"])\n",
    "\n",
    "    print(\"\\n[OK] logs crudos guardados:\", flush=True)\n",
    "    print(\" -\", raw_path_parquet, flush=True)\n",
    "    print(\" -\", raw_path_csv, flush=True)\n",
    "    print(\"[OK] filas únicas inferidas:\", len(raw_df), flush=True)\n",
    "\n",
    "finally:\n",
    "    # Intentar apagar servidor limpiamente\n",
    "    try:\n",
    "        requests.post(f\"{base_url}/control\", json={\"cmd\": \"shutdown\"}, timeout=5)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        server_thread.shutdown()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6d5222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:36:04.113497Z",
     "iopub.status.busy": "2026-02-19T17:36:04.113349Z",
     "iopub.status.idle": "2026-02-19T17:37:23.250134Z",
     "shell.execute_reply": "2026-02-19T17:37:23.249806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Calculando métricas por modelo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FIG] Guardada matriz de confusión: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/report/figures/confusion_battery_active_power_any-to-80_100.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FIG] Guardada matriz de confusión: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/report/figures/confusion_battery_active_power_set_response_any-to-80_100.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FIG] Guardada matriz de confusión: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/report/figures/confusion_pvpcs_active_power_any-to-80_100.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FIG] Guardada matriz de confusión: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/report/figures/confusion_ge_active_power_any-to-80_100.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Métricas guardadas en /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/metrics/metrics_per_model.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Reporte HTML: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/report/report.html\n"
     ]
    }
   ],
   "source": [
    "if raw_df is None or raw_df.empty:\n",
    "    print(\"[WARN] raw_df vacío; no se pueden calcular métricas.\")\n",
    "else:\n",
    "    print(\"\\n[INFO] Calculando métricas por modelo...\", flush=True)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for m in manifest[\"models\"]:\n",
    "        pred_name = m[\"prediction_name\"]\n",
    "        dataset_path = m[\"dataset_path\"]\n",
    "\n",
    "        df_model = pd.read_parquet(dataset_path)\n",
    "        if sample_size:\n",
    "            df_model = df_model.head(sample_size)\n",
    "\n",
    "        # Serializar ventana para merge\n",
    "        df_model[\"window\"] = df_model[\"OW_events\"].apply(\n",
    "            lambda w: json.dumps(\n",
    "                to_json_safe_window(w),\n",
    "                separators=(\",\", \":\"),\n",
    "                ensure_ascii=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        model_log = raw_df[raw_df[\"prediction_name\"] == pred_name][[\"window\", \"y_pred\"]]\n",
    "\n",
    "        merged = df_model.merge(model_log, on=\"window\", how=\"left\")\n",
    "\n",
    "        valid = merged[merged[\"y_pred\"].notna()].copy()\n",
    "        if valid.empty:\n",
    "            print(f\"[WARN] Sin predicciones válidas para {pred_name}, se omite.\", flush=True)\n",
    "            continue\n",
    "\n",
    "        valid[\"y_pred\"] = valid[\"y_pred\"].astype(int)\n",
    "\n",
    "        tp = ((valid[\"label\"] == 1) & (valid[\"y_pred\"] == 1)).sum()\n",
    "        tn = ((valid[\"label\"] == 0) & (valid[\"y_pred\"] == 0)).sum()\n",
    "        fp = ((valid[\"label\"] == 0) & (valid[\"y_pred\"] == 1)).sum()\n",
    "        fn = ((valid[\"label\"] == 1) & (valid[\"y_pred\"] == 0)).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "        metrics.append({\n",
    "            \"prediction_name\": pred_name,\n",
    "            \"tp\": int(tp),\n",
    "            \"tn\": int(tn),\n",
    "            \"fp\": int(fp),\n",
    "            \"fn\": int(fn),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1\": float(f1),\n",
    "        })\n",
    "\n",
    "        # Figura de matriz de confusión\n",
    "        plt.figure()\n",
    "        plt.imshow([[tn, fp], [fn, tp]])\n",
    "        plt.title(f\"Confusion Matrix - {pred_name}\")\n",
    "        plt.colorbar()\n",
    "        plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n",
    "        plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n",
    "        plt.tight_layout()\n",
    "        fig_path = figures_dir / f\"confusion_{pred_name}.png\"\n",
    "        plt.savefig(fig_path)\n",
    "        plt.close()\n",
    "        print(f\"[FIG] Guardada matriz de confusión: {fig_path}\", flush=True)\n",
    "\n",
    "    if metrics:\n",
    "        metrics_df = pd.DataFrame(metrics)\n",
    "        metrics_csv_path = metrics_dir / \"metrics_per_model.csv\"\n",
    "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "        print(f\"[OK] Métricas guardadas en {metrics_csv_path}\", flush=True)\n",
    "\n",
    "        # Informe HTML simple\n",
    "        report_html = \"<html><body><h1>F07 Report</h1>\"\n",
    "        report_html += \"<h2>Métricas por modelo</h2>\"\n",
    "        report_html += metrics_df.to_html(index=False)\n",
    "        report_html += \"</body></html>\"\n",
    "\n",
    "        report_path = report_dir / \"report.html\"\n",
    "        report_path.write_text(report_html, encoding=\"utf-8\")\n",
    "        print(f\"[OK] Reporte HTML: {report_path}\", flush=True)\n",
    "    else:\n",
    "        print(\"[WARN] No se generaron métricas.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff3beb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:37:23.252053Z",
     "iopub.status.busy": "2026-02-19T17:37:23.251860Z",
     "iopub.status.idle": "2026-02-19T17:37:23.360418Z",
     "shell.execute_reply": "2026-02-19T17:37:23.359930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRACE] Metadata de F07 escrita en /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/07_deployrun/v701/07_deployrun_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Registrar trazabilidad de la fase\n",
    "metadata_path = variant_root / f\"{PHASE}_metadata.json\"\n",
    "\n",
    "write_metadata(\n",
    "    stage=PHASE,\n",
    "    variant=VARIANT,\n",
    "    parent_variant=manifest.get(\"f06_variant\", parent_f06),\n",
    "    inputs=[str(manifest_path)],\n",
    "    outputs=[str(raw_path_parquet), str(metrics_dir), str(report_dir)],\n",
    "    params=params,\n",
    "    metadata_path=metadata_path,\n",
    ")\n",
    "\n",
    "print(f\"[TRACE] Metadata de F07 escrita en {metadata_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
