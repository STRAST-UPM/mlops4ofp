{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 05 — Modeling\n",
    "\n",
    "Notebook **ejecutable celda a celda**, **equivalente funcionalmente** a `05_modeling.py`.\n",
    "\n",
    "Contrato:\n",
    "- Ejecutar todas las celdas en orden ⇔ ejecutar la script completa.\n",
    "- Mismos parámetros, mismos artefactos, misma trazabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuración de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT = \"vXXX\"  # Cambiar por la variante real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports y estabilización del runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from time import perf_counter\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '1'\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrap del proyecto e imports internos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path.cwd().resolve()\n",
    "ROOT = NOTEBOOK_PATH\n",
    "for _ in range(10):\n",
    "    if (ROOT / 'mlops4ofp').exists():\n",
    "        break\n",
    "    ROOT = ROOT.parent\n",
    "else:\n",
    "    raise RuntimeError('No se pudo localizar project root')\n",
    "sys.path.insert(0, str(ROOT))\n",
    "from mlops4ofp.tools.run_context import detect_execution_dir, detect_project_root, assemble_run_context, print_run_context\n",
    "from mlops4ofp.tools.params_manager import ParamsManager\n",
    "from mlops4ofp.tools.traceability import write_metadata\n",
    "from mlops4ofp.tools.artifacts import get_git_hash\n",
    "from mlops4ofp.tools.figures import save_figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilidades generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y):\n",
    "    pos = (y == 1).sum()\n",
    "    neg = (y == 0).sum()\n",
    "    if pos == 0:\n",
    "        return None\n",
    "    return {0: 1.0, 1: neg / pos}\n",
    "\n",
    "def pad_sequences(seqs, max_len, pad_value=0):\n",
    "    out = np.full((len(seqs), max_len), pad_value, dtype=np.int32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        if not s:\n",
    "            continue\n",
    "        trunc = s[-max_len:]\n",
    "        out[i, -len(trunc):] = trunc\n",
    "    return out\n",
    "\n",
    "def check_split_feasibility(y_train, y_val):\n",
    "    issues = []\n",
    "    if y_train.sum() == 0:\n",
    "        issues.append('train split sin positivos')\n",
    "    if y_val.sum() == 0:\n",
    "        issues.append('val split sin positivos')\n",
    "    return issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Familias de modelos (contrato F05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dense_bow(df, params):\n",
    "    sequences = df['OW_events'].tolist()\n",
    "    y = df['label'].values.astype(np.int32)\n",
    "    vocab = sorted(set(ev for s in sequences for ev in s))\n",
    "    index = {ev: i for i, ev in enumerate(vocab)}\n",
    "    X = np.zeros((len(sequences), len(vocab)), dtype=np.float32)\n",
    "    for i, s in enumerate(sequences):\n",
    "        for ev in s:\n",
    "            X[i, index[ev]] += 1.0\n",
    "    return X, y, {'input_dim': X.shape[1]}\n",
    "\n",
    "def build_dense_bow_model(aux, hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(aux['input_dim'],)))\n",
    "    for _ in range(hp['n_layers']):\n",
    "        model.add(layers.Dense(hp['units'], activation='relu'))\n",
    "        if hp['dropout'] > 0:\n",
    "            model.add(layers.Dropout(hp['dropout']))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def vectorize_sequence(df, params):\n",
    "    sequences = df['OW_events'].tolist()\n",
    "    y = df['label'].values.astype(np.int32)\n",
    "    vocab = sorted(set(ev for s in sequences for ev in s))\n",
    "    index = {ev: i + 1 for i, ev in enumerate(vocab)}\n",
    "    seqs_idx = [[index[e] for e in s] for s in sequences]\n",
    "    max_len = int(np.percentile([len(s) for s in seqs_idx], 95))\n",
    "    X = pad_sequences(seqs_idx, max_len)\n",
    "    return X, y, {'vocab_size': len(vocab), 'max_len': max_len}\n",
    "\n",
    "def build_sequence_embedding_model(aux, hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(aux['max_len'],)))\n",
    "    model.add(layers.Embedding(aux['vocab_size'] + 1, hp['embed_dim'], mask_zero=True))\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    for _ in range(hp['n_layers']):\n",
    "        model.add(layers.Dense(hp['units'], activation='relu'))\n",
    "        if hp['dropout'] > 0:\n",
    "            model.add(layers.Dropout(hp['dropout']))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def build_cnn1d_model(aux, hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(aux['max_len'],)))\n",
    "    model.add(layers.Embedding(aux['vocab_size'] + 1, hp['embed_dim'], mask_zero=False))\n",
    "    model.add(layers.Conv1D(hp['filters'], hp['kernel_size'], activation='relu', padding='same'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    for _ in range(hp['n_layers']):\n",
    "        model.add(layers.Dense(hp['units'], activation='relu'))\n",
    "        if hp['dropout'] > 0:\n",
    "            model.add(layers.Dropout(hp['dropout']))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "MODEL_FAMILIES = {\n",
    "    'dense_bow': {'vectorize': vectorize_dense_bow, 'build_model': build_dense_bow_model},\n",
    "    'sequence_embedding': {'vectorize': vectorize_sequence, 'build_model': build_sequence_embedding_model},\n",
    "    'cnn1d': {'vectorize': vectorize_sequence, 'build_model': build_cnn1d_model},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inicialización de fase y carga de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE = '05_modeling'\n",
    "t_start = perf_counter()\n",
    "execution_dir = detect_execution_dir()\n",
    "project_root = detect_project_root(execution_dir)\n",
    "pm = ParamsManager(PHASE, project_root)\n",
    "pm.set_current(VARIANT)\n",
    "variant_root = pm.current_variant_dir()\n",
    "ctx = assemble_run_context(execution_dir, project_root, PHASE, VARIANT, variant_root)\n",
    "print_run_context(ctx)\n",
    "with open(variant_root / 'params.yaml', 'r', encoding='utf-8') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "parent_variant_f04 = params['parent_variant']\n",
    "model_family = params['model_family']\n",
    "family = MODEL_FAMILIES[model_family]\n",
    "vectorize_fn = family['vectorize']\n",
    "build_model_fn = family['build_model']\n",
    "automl_cfg = params['automl']\n",
    "training_cfg = params['training']\n",
    "imbalance_cfg = params['imbalance']\n",
    "eval_cfg = params['evaluation']\n",
    "cand_cfg = params['candidate_selection']\n",
    "search_space = params['search_space'][model_family]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semillas y carga del dataset F04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = automl_cfg.get('seed', 42)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "input_dataset_path = project_root / 'executions' / '04_targetengineering' / parent_variant_f04 / '04_targetengineering_dataset.parquet'\n",
    "df = pd.read_parquet(input_dataset_path)\n",
    "X, y, aux = vectorize_fn(df, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Split train / val / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "n = len(idx)\n",
    "n_train = int(eval_cfg['split']['train'] * n)\n",
    "n_val = int(eval_cfg['split']['val'] * n)\n",
    "train_idx = idx[:n_train]\n",
    "val_idx = idx[n_train:n_train+n_val]\n",
    "test_idx = idx[n_train+n_val:]\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "max_samples = training_cfg.get('max_samples')\n",
    "if max_samples is not None and len(X_train) > max_samples:\n",
    "    sel = np.random.choice(len(X_train), size=max_samples, replace=False)\n",
    "    X_train, y_train = X_train[sel], y_train[sel]\n",
    "pd.DataFrame(X_train).to_parquet(variant_root / 'train.parquet')\n",
    "pd.DataFrame(X_val).to_parquet(variant_root / 'val.parquet')\n",
    "pd.DataFrame(X_test).to_parquet(variant_root / 'test.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weights(y_train) if imbalance_cfg['strategy']=='auto' else None\n",
    "issues = check_split_feasibility(y_train, y_val)\n",
    "if issues:\n",
    "    pm.save_metadata({'phase': PHASE, 'variant': VARIANT, 'status': 'skipped', 'reason': issues})\n",
    "    raise RuntimeError('Dataset no viable')\n",
    "experiments_dir = variant_root / 'experiments'\n",
    "experiments_dir.mkdir(exist_ok=True)\n",
    "runs = []\n",
    "steps_per_epoch = min(max(1, len(X_train)//training_cfg['batch_size']), 2000)\n",
    "for trial in range(automl_cfg['max_trials']):\n",
    "    hp = {k: random.choice(v) for k, v in search_space.items()}\n",
    "    model = build_model_fn(aux, hp)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp['learning_rate']), loss='binary_crossentropy', metrics=[keras.metrics.Recall(name='recall')])\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=training_cfg['epochs'], batch_size=hp['batch_size'], class_weight=class_weights, verbose=1, steps_per_epoch=steps_per_epoch)\n",
    "    val_recall = max(hist.history['val_recall'])\n",
    "    exp_id = f\"exp_{trial:03d}\"\n",
    "    exp_dir = experiments_dir / exp_id\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    model.save(exp_dir / 'model.h5')\n",
    "    with open(exp_dir / 'metrics.json', 'w') as f:\n",
    "        json.dump({'val_recall': val_recall}, f, indent=2)\n",
    "    runs.append({'exp_id': exp_id, 'val_recall': val_recall, 'hp': hp})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Selección de candidatos y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = cand_cfg['threshold']\n",
    "selected = [r for r in runs if r['val_recall'] >= threshold]\n",
    "if not selected:\n",
    "    selected = [max(runs, key=lambda r: r['val_recall'])]\n",
    "candidates_dir = variant_root / 'candidates'\n",
    "candidates_dir.mkdir(exist_ok=True)\n",
    "fig_dir = variant_root / 'figures'\n",
    "fig_dir.mkdir(exist_ok=True)\n",
    "candidates = []\n",
    "for i, r in enumerate(sorted(selected, key=lambda x: -x['val_recall']), 1):\n",
    "    cand_id = f\"cand_{i:02d}\"\n",
    "    cand_dir = candidates_dir / cand_id\n",
    "    cand_dir.mkdir(exist_ok=True)\n",
    "    model = keras.models.load_model(experiments_dir / r['exp_id'] / 'model.h5')\n",
    "    model.save(cand_dir / 'model.h5')\n",
    "    y_score = model.predict(X_test).ravel()\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "    y_pred = (y_score >= 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    with open(cand_dir / 'metrics.json', 'w') as f:\n",
    "        json.dump({'val_recall': r['val_recall'], 'tp': int(tp), 'tn': int(tn), 'fp': int(fp), 'fn': int(fn)}, f, indent=2)\n",
    "    save_figure(fig_dir / f\"{cand_id}_precision_recall.png\", lambda: (plt.plot(recall, precision), plt.xlabel('Recall'), plt.ylabel('Precision'), plt.title(f\"{cand_id} — Precision–Recall\")))\n",
    "    save_figure(fig_dir / f\"{cand_id}_recall_vs_threshold.png\", lambda: (plt.plot(thresholds, recall[:-1]), plt.xlabel('Threshold'), plt.ylabel('Recall'), plt.title(f\"{cand_id} — Recall vs Threshold\")))\n",
    "    candidates.append({'candidate_id': cand_id, 'from_experiment': r['exp_id'], 'val_recall': r['val_recall'], 'confusion_matrix': {'tp': int(tp), 'tn': int(tn), 'fp': int(fp), 'fn': int(fn)}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10d29d",
   "metadata": {},
   "source": [
    "## 9bis. Selección del modelo único mejor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fc030",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = sorted(\n",
    "    candidates,\n",
    "    key=lambda c: (\n",
    "        -c[\"val_recall\"],\n",
    "        c[\"confusion_matrix\"][\"fn\"],\n",
    "        c[\"confusion_matrix\"][\"fp\"],\n",
    "    ),\n",
    ")[0]\n",
    "\n",
    "best_dir = variant_root / \"best\"\n",
    "best_dir.mkdir(exist_ok=True)\n",
    "\n",
    "src_dir = candidates_dir / best[\"candidate_id\"]\n",
    "\n",
    "# Copiar modelo\n",
    "model = keras.models.load_model(src_dir / \"model.h5\")\n",
    "model.save(best_dir / \"model.h5\")\n",
    "\n",
    "# Copiar métricas\n",
    "with open(src_dir / \"metrics.json\", \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "with open(best_dir / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# Guardar trazabilidad de origen\n",
    "with open(best_dir / \"origin.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"selected_from\": best[\"candidate_id\"],\n",
    "            \"selection_rule\": \"max(val_recall), min(fn), min(fp)\",\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Metadata y trazabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"phase\": PHASE,\n",
    "    \"variant\": VARIANT,\n",
    "    \"parent_variant\": parent_variant_f04,\n",
    "    \"model_family\": model_family,\n",
    "    \"num_experiments\": len(runs),\n",
    "    \"num_candidates\": len(candidates),\n",
    "    \"candidates\": candidates,\n",
    "    \"best_model\": {\n",
    "        \"candidate_id\": best[\"candidate_id\"],\n",
    "        \"path\": str(best_dir / \"model.h5\"),\n",
    "    },\n",
    "    \"selection_policy\": cand_cfg,\n",
    "    \"git\": {\"commit\": get_git_hash()},\n",
    "    \"generated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "}\n",
    "\n",
    "metadata_path = variant_root / f\"{PHASE}_metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "write_metadata(\n",
    "    stage=PHASE,\n",
    "    variant=VARIANT,\n",
    "    parent_variant=parent_variant_f04,\n",
    "    inputs=[str(input_dataset_path)],\n",
    "    outputs=[str(metadata_path)],\n",
    "    params=params,\n",
    "    metadata_path=metadata_path,\n",
    ")\n",
    "\n",
    "print(f\"[DONE] Fase 05 completada en {perf_counter()-t_start:.1f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
