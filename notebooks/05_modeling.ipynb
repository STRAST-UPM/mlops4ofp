{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bdb6d9",
   "metadata": {},
   "source": [
    "# Fase 05 — Modeling\n",
    "\n",
    "Notebook ejecutable y **equivalente** a `scripts/05_modeling.py`.\n",
    "\n",
    "**Importante:** este notebook **no** gestiona MLflow. El registro MLflow se realiza en Makefile (`publish5`/`remove5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e68aee",
   "metadata": {},
   "source": [
    "## 1) Código de la fase (idéntico al script)\n",
    "\n",
    "La celda siguiente contiene el código completo de la fase, sin el bloque `argparse` final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2651a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T17:57:52.148327Z",
     "iopub.status.busy": "2026-02-18T17:57:52.148099Z",
     "iopub.status.idle": "2026-02-18T17:57:55.413129Z",
     "shell.execute_reply": "2026-02-18T17:57:55.412714Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fase 05 — Modeling\n",
    "\n",
    "Entrena modelos para una única familia por variante.\n",
    "\n",
    "Produce:\n",
    "- experiments/              → auditoría de trials\n",
    "- model_final.h5            → modelo único seleccionado\n",
    "- splits.parquet            → índices train/val/test\n",
    "- 05_modeling_metadata.json → metadata enriquecida\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from time import perf_counter\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# ============================================================\n",
    "# TensorFlow runtime stabilization\n",
    "# ============================================================\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import legacy as legacy_optimizers\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BOOTSTRAP\n",
    "# ============================================================\n",
    "SCRIPT_PATH = Path.cwd().resolve()\n",
    "ROOT = SCRIPT_PATH\n",
    "for _ in range(10):\n",
    "    if (ROOT / \"mlops4ofp\").exists():\n",
    "        break\n",
    "    ROOT = ROOT.parent\n",
    "else:\n",
    "    raise RuntimeError(\"No se pudo localizar project root\")\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from mlops4ofp.tools.run_context import (\n",
    "    detect_execution_dir,\n",
    "    detect_project_root,\n",
    "    assemble_run_context,\n",
    "    print_run_context,\n",
    ")\n",
    "from mlops4ofp.tools.params_manager import ParamsManager\n",
    "from mlops4ofp.tools.traceability import write_metadata\n",
    "from mlops4ofp.tools.artifacts import get_git_hash\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UTILIDADES\n",
    "# ============================================================\n",
    "\n",
    "def compute_class_weights(y):\n",
    "    pos = np.sum(y == 1)\n",
    "    neg = np.sum(y == 0)\n",
    "    if pos == 0:\n",
    "        return None\n",
    "    return {0: 1.0, 1: neg / pos}\n",
    "\n",
    "\n",
    "def convert_to_native_types(obj):\n",
    "    \"\"\"Convierte tipos numpy a tipos nativos de Python para serialización JSON.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native_types(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def apply_rare_events(df, imbalance_cfg, seed):\n",
    "    strategy = imbalance_cfg.get(\"strategy\", \"none\")\n",
    "\n",
    "    if strategy != \"rare_events\":\n",
    "        return df, {\"strategy\": \"none\"}\n",
    "\n",
    "    max_majority = imbalance_cfg.get(\"max_majority_samples\")\n",
    "\n",
    "    if max_majority is None:\n",
    "        return df, {\n",
    "            \"strategy\": \"rare_events\",\n",
    "            \"note\": \"max_majority_samples=None → no reducción aplicada\"\n",
    "        }\n",
    "\n",
    "    df_pos = df[df[\"label\"] == 1]\n",
    "    df_neg = df[df[\"label\"] == 0]\n",
    "\n",
    "    n_pos_before = len(df_pos)\n",
    "    n_neg_before = len(df_neg)\n",
    "\n",
    "    n_neg_sample = min(max_majority, n_neg_before)\n",
    "    df_neg_sample = df_neg.sample(n=n_neg_sample, random_state=seed)\n",
    "\n",
    "    df_new = pd.concat([df_pos, df_neg_sample])\n",
    "    df_new = df_new.sample(frac=1.0, random_state=seed)\n",
    "\n",
    "    info = {\n",
    "        \"strategy\": \"rare_events\",\n",
    "        \"n_pos_before\": int(n_pos_before),\n",
    "        \"n_neg_before\": int(n_neg_before),\n",
    "        \"n_pos_after\": int(len(df_pos)),\n",
    "        \"n_neg_after\": int(n_neg_sample),\n",
    "    }\n",
    "\n",
    "    return df_new, info\n",
    "\n",
    "\n",
    "def pad_sequences(seqs, max_len, pad_value=0):\n",
    "    out = np.full((len(seqs), max_len), pad_value, dtype=np.int32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        trunc = s[-max_len:]\n",
    "        if len(trunc) == 0:\n",
    "            continue\n",
    "        out[i, -len(trunc):] = trunc\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAMILIAS\n",
    "# ============================================================\n",
    "\n",
    "def vectorize_dense_bow(df):\n",
    "    sequences = df[\"OW_events\"].tolist()\n",
    "    y = df[\"label\"].values.astype(np.int32)\n",
    "\n",
    "    vocab = sorted(set(ev for s in sequences for ev in s))\n",
    "    index = {ev: i for i, ev in enumerate(vocab)}\n",
    "\n",
    "    X = np.zeros((len(sequences), len(vocab)), dtype=np.float32)\n",
    "    for i, s in enumerate(sequences):\n",
    "        for ev in s:\n",
    "            X[i, index[ev]] += 1.0\n",
    "\n",
    "    return X, y, {\n",
    "        \"input_dim\": X.shape[1],\n",
    "        \"vocab\": vocab,\n",
    "        \"vectorization\": \"dense_bow\"\n",
    "    }\n",
    "\n",
    "\n",
    "def build_dense_bow_model(aux, hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(aux[\"input_dim\"],)))\n",
    "\n",
    "    for _ in range(hp[\"n_layers\"]):\n",
    "        model.add(layers.Dense(hp[\"units\"], activation=\"relu\"))\n",
    "        if hp[\"dropout\"] > 0:\n",
    "            model.add(layers.Dropout(hp[\"dropout\"]))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vectorize_sequence(df):\n",
    "    sequences = df[\"OW_events\"].tolist()\n",
    "    y = df[\"label\"].values.astype(np.int32)\n",
    "\n",
    "    vocab = sorted(set(ev for s in sequences for ev in s))\n",
    "    index = {ev: i + 1 for i, ev in enumerate(vocab)}\n",
    "\n",
    "    seqs_idx = [[index[e] for e in s] for s in sequences]\n",
    "    lengths = [len(s) for s in seqs_idx]\n",
    "    max_len = max(1, int(np.percentile(lengths, 95))) if lengths else 1\n",
    "    X = pad_sequences(seqs_idx, max_len)\n",
    "\n",
    "    return X, y, {\n",
    "        \"vocab\": vocab,\n",
    "        \"vocab_size\": len(vocab),\n",
    "        \"max_len\": max_len,\n",
    "        \"vectorization\": \"sequence\"\n",
    "    }\n",
    "\n",
    "\n",
    "def build_sequence_embedding_model(aux, hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(aux[\"max_len\"],)))\n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=aux[\"vocab_size\"] + 1,\n",
    "        output_dim=hp[\"embed_dim\"],\n",
    "        mask_zero=True,\n",
    "    ))\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "    for _ in range(hp[\"n_layers\"]):\n",
    "        model.add(layers.Dense(hp[\"units\"], activation=\"relu\"))\n",
    "        if hp[\"dropout\"] > 0:\n",
    "            model.add(layers.Dropout(hp[\"dropout\"]))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_cnn1d_model(aux, hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(aux[\"max_len\"],)))\n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=aux[\"vocab_size\"] + 1,\n",
    "        output_dim=hp[\"embed_dim\"],\n",
    "    ))\n",
    "    model.add(layers.Conv1D(\n",
    "        filters=hp[\"filters\"],\n",
    "        kernel_size=hp[\"kernel_size\"],\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\",\n",
    "    ))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "    for _ in range(hp[\"n_layers\"]):\n",
    "        model.add(layers.Dense(hp[\"units\"], activation=\"relu\"))\n",
    "        if hp[\"dropout\"] > 0:\n",
    "            model.add(layers.Dropout(hp[\"dropout\"]))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "FAMILIES = {\n",
    "    \"dense_bow\": {\n",
    "        \"vectorize\": vectorize_dense_bow,\n",
    "        \"build\": build_dense_bow_model,\n",
    "    },\n",
    "    \"sequence_embedding\": {\n",
    "        \"vectorize\": vectorize_sequence,\n",
    "        \"build\": build_sequence_embedding_model,\n",
    "    },\n",
    "    \"cnn1d\": {\n",
    "        \"vectorize\": vectorize_sequence,\n",
    "        \"build\": build_cnn1d_model,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MÉTRICAS / UMBRAL / LOOP\n",
    "# ============================================================\n",
    "\n",
    "def binary_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(np.int32)\n",
    "\n",
    "    TP = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    TN = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    FP = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    FN = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-9)\n",
    "    recall = TP / (TP + FN + 1e-9)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "    accuracy = (TP + TN) / max(TP + TN + FP + FN, 1)\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"TP\": TP,\n",
    "        \"TN\": TN,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "    }\n",
    "\n",
    "\n",
    "def best_threshold_by_f1(y_true, y_prob):\n",
    "    best = None\n",
    "    for t in np.linspace(0.05, 0.95, 19):\n",
    "        m = binary_metrics(y_true, y_prob, threshold=t)\n",
    "        if best is None or m[\"f1\"] > best[\"f1\"]:\n",
    "            best = m\n",
    "    return best\n",
    "\n",
    "\n",
    "def sample_hparams(search_space):\n",
    "    hp = {}\n",
    "    for k, v in search_space.items():\n",
    "        if not isinstance(v, list):\n",
    "            hp[k] = v\n",
    "        else:\n",
    "            hp[k] = random.choice(v)\n",
    "    return hp\n",
    "\n",
    "\n",
    "def train_one_trial(\n",
    "    family_name,\n",
    "    family_cfg,\n",
    "    hp,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    training_cfg,\n",
    "    class_weight,\n",
    "    trial_id,\n",
    "    model_dir,\n",
    "    seed,\n",
    "    verbose=0,\n",
    "    should_save_model=False,\n",
    "    callbacks=None,\n",
    "    ):\n",
    "\n",
    "    tf.keras.utils.set_random_seed(seed + trial_id)\n",
    "    model = family_cfg[\"build\"](family_cfg[\"aux\"], hp)\n",
    "\n",
    "    lr = hp.get(\"lr\", 1e-3)\n",
    "    opt = legacy_optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "\n",
    "    batch_size = int(hp.get(\"batch_size\", 64))\n",
    "    epochs = int(training_cfg.get(\"epochs\", 5))\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        class_weight=class_weight,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    val_prob = model.predict(X_val, verbose=0).reshape(-1)\n",
    "    best_val = best_threshold_by_f1(y_val, val_prob)\n",
    "\n",
    "    return {\n",
    "        \"trial_id\": trial_id,\n",
    "        \"hparams\": hp,\n",
    "        \"val_metrics\": best_val,\n",
    "        \"history\": history.history,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main(variant: str):\n",
    "    execution_dir = detect_execution_dir()\n",
    "    project_root = detect_project_root(execution_dir)\n",
    "    phase = \"05_modeling\"\n",
    "\n",
    "    pm = ParamsManager(phase, project_root)\n",
    "    pm.set_current(variant)\n",
    "    variant_root = pm.current_variant_dir()\n",
    "\n",
    "    ctx = assemble_run_context(\n",
    "        execution_dir=execution_dir,\n",
    "        project_root=project_root,\n",
    "        phase=phase,\n",
    "        variant=variant,\n",
    "        variant_root=variant_root,\n",
    "    )\n",
    "    print_run_context(ctx)\n",
    "\n",
    "    params_path = ctx[\"variant_root\"] / \"params.yaml\"\n",
    "    with open(params_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        params = yaml.safe_load(f) or {}\n",
    "    ctx[\"variant_params\"] = params\n",
    "\n",
    "    parent_variant = params[\"parent_variant\"]\n",
    "    parent_dataset = project_root / \"executions\" / \"04_targetengineering\" / parent_variant / \"04_targetengineering_dataset.parquet\"\n",
    "    if not parent_dataset.exists():\n",
    "        raise FileNotFoundError(f\"No existe dataset padre: {parent_dataset}\")\n",
    "\n",
    "    df = pd.read_parquet(parent_dataset)\n",
    "\n",
    "    if \"label\" not in df.columns:\n",
    "        raise RuntimeError(\"El dataset de F04 debe contener columna 'label'.\")\n",
    "    if \"OW_events\" not in df.columns:\n",
    "        raise RuntimeError(\"El dataset de F04 debe contener columna 'OW_events'.\")\n",
    "\n",
    "    seed = int(params.get(\"training\", {}).get(\"seed\", 42))\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    imbalance_cfg = params.get(\"imbalance\", {})\n",
    "    df, imbalance_info = apply_rare_events(df, imbalance_cfg, seed)\n",
    "\n",
    "    max_samples = params.get(\"training\", {}).get(\"max_samples\")\n",
    "    if max_samples is not None and len(df) > max_samples:\n",
    "        df = df.sample(n=max_samples, random_state=seed)\n",
    "\n",
    "    family = params[\"model_family\"]\n",
    "    if family not in FAMILIES:\n",
    "        raise ValueError(f\"model_family '{family}' no soportada\")\n",
    "\n",
    "    family_cfg = FAMILIES[family]\n",
    "    vectorize_fn = family_cfg[\"vectorize\"]\n",
    "\n",
    "    X, y, aux = vectorize_fn(df)\n",
    "\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    n = len(X)\n",
    "    n_train = int(0.7 * n)\n",
    "    n_val = int(0.15 * n)\n",
    "\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
    "    X_test, y_test = X[n_train+n_val:], y[n_train+n_val:]\n",
    "\n",
    "    experiments_dir = ctx[\"variant_root\"] / \"experiments\"\n",
    "    models_dir = experiments_dir / \"models\"\n",
    "    experiments_dir.mkdir(parents=True, exist_ok=True)\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"split\": [\"train\"] * len(X_train) + [\"val\"] * len(X_val) + [\"test\"] * len(X_test),\n",
    "        \"idx\": list(range(len(X_train))) + list(range(len(X_train), len(X_train)+len(X_val))) + list(range(len(X_train)+len(X_val), n))\n",
    "    }).to_parquet(experiments_dir / \"splits.parquet\", index=False)\n",
    "\n",
    "    class_weight = None\n",
    "    if params.get(\"training\", {}).get(\"class_weight_mode\") == \"balanced\":\n",
    "        class_weight = compute_class_weights(y_train)\n",
    "\n",
    "    automl_cfg = params.get(\"automl\", {})\n",
    "    n_trials = int(automl_cfg.get(\"n_trials\", 1))\n",
    "    search_space = params.get(\"search_space\", {})\n",
    "    search_space.setdefault(\"batch_size\", [params.get(\"training\", {}).get(\"batch_size\", 64)])\n",
    "    search_space.setdefault(\"lr\", [params.get(\"training\", {}).get(\"lr\", 1e-3)])\n",
    "    search_space.setdefault(\"n_layers\", [1])\n",
    "    search_space.setdefault(\"units\", [64])\n",
    "    search_space.setdefault(\"dropout\", [0.0])\n",
    "\n",
    "    if family in {\"sequence_embedding\", \"cnn1d\"}:\n",
    "        search_space.setdefault(\"embed_dim\", [32])\n",
    "    if family == \"cnn1d\":\n",
    "        search_space.setdefault(\"filters\", [64])\n",
    "        search_space.setdefault(\"kernel_size\", [3])\n",
    "\n",
    "    family_cfg_runtime = dict(family_cfg)\n",
    "    family_cfg_runtime[\"aux\"] = aux\n",
    "\n",
    "    trials_rows = []\n",
    "    best_trial = None\n",
    "\n",
    "    start_all = perf_counter()\n",
    "\n",
    "    # Callback Early Stopping configurable\n",
    "    callbacks = []\n",
    "    es_cfg = params.get(\"training\", {}).get(\"early_stopping\", {})\n",
    "    if es_cfg.get(\"enabled\", False):\n",
    "        callbacks.append(\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=es_cfg.get(\"monitor\", \"val_loss\"),\n",
    "                patience=int(es_cfg.get(\"patience\", 3)),\n",
    "                restore_best_weights=bool(es_cfg.get(\"restore_best_weights\", True)),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for trial_id in range(1, n_trials + 1):\n",
    "        hp = sample_hparams(search_space)\n",
    "\n",
    "        t0 = perf_counter()\n",
    "        result = train_one_trial(\n",
    "            family,\n",
    "            family_cfg_runtime,\n",
    "            hp,\n",
    "            X_train, y_train,\n",
    "            X_val, y_val,\n",
    "            params.get(\"training\", {}),\n",
    "            class_weight,\n",
    "            trial_id,\n",
    "            models_dir,\n",
    "            seed,\n",
    "            verbose=0,\n",
    "            should_save_model=False,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        dt = perf_counter() - t0\n",
    "\n",
    "        valm = result[\"val_metrics\"]\n",
    "\n",
    "        row = {\n",
    "            \"trial_id\": trial_id,\n",
    "            \"family\": family,\n",
    "            \"seconds\": float(dt),\n",
    "            \"val_f1\": valm[\"f1\"],\n",
    "            \"val_precision\": valm[\"precision\"],\n",
    "            \"val_recall\": valm[\"recall\"],\n",
    "            \"val_acc\": valm[\"accuracy\"],\n",
    "            \"val_threshold\": valm[\"threshold\"],\n",
    "            \"hparams\": json.dumps(result[\"hparams\"], ensure_ascii=False),\n",
    "        }\n",
    "        trials_rows.append(row)\n",
    "\n",
    "        if best_trial is None or valm[\"f1\"] > best_trial[\"val_metrics\"][\"f1\"]:\n",
    "            best_trial = result\n",
    "\n",
    "    trials_df = pd.DataFrame(trials_rows).sort_values(\"val_f1\", ascending=False)\n",
    "    trials_df.to_parquet(experiments_dir / \"trials.parquet\", index=False)\n",
    "\n",
    "    best_model = best_trial[\"model\"]\n",
    "    best_hp = best_trial[\"hparams\"]\n",
    "    best_val = best_trial[\"val_metrics\"]\n",
    "\n",
    "    y_test_prob = best_model.predict(X_test, verbose=0).reshape(-1)\n",
    "    test_metrics = binary_metrics(y_test, y_test_prob, threshold=best_val[\"threshold\"] )\n",
    "\n",
    "    model_final_path = ctx[\"variant_root\"] / \"model_final.h5\"\n",
    "    best_model.save(model_final_path)\n",
    "\n",
    "    family_cfg_runtime[\"aux\"] = convert_to_native_types(aux)\n",
    "\n",
    "    metadata = {\n",
    "        \"stage\": phase,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"variant\": variant,\n",
    "        \"parent_variant\": parent_variant,\n",
    "        \"model_family\": family,\n",
    "        \"inputs\": [str(parent_dataset)],\n",
    "        \"outputs\": [str(model_final_path), str(experiments_dir / \"trials.parquet\")],\n",
    "        \"params\": {\n",
    "            \"search_space\": search_space,\n",
    "            \"n_trials\": n_trials,\n",
    "            \"training\": params.get(\"training\", {}),\n",
    "            \"imbalance\": imbalance_cfg,\n",
    "            \"vectorization\": aux,          # ← incluye vocab / max_len / input_dim\n",
    "            \"best_hparams\": best_hp,\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"best_val\": best_val,\n",
    "            \"test\": test_metrics,\n",
    "            \"n_rows\": int(len(df)),\n",
    "            \"n_train\": int(len(X_train)),\n",
    "            \"n_val\": int(len(X_val)),\n",
    "            \"n_test\": int(len(X_test)),\n",
    "            \"class_balance\": {\n",
    "                \"pos\": int(np.sum(y == 1)),\n",
    "                \"neg\": int(np.sum(y == 0)),\n",
    "            },\n",
    "            \"runtime_seconds_total\": float(perf_counter() - start_all),\n",
    "            \"imbalance_info\": imbalance_info,\n",
    "        },\n",
    "        \"git\": {\"commit\": get_git_hash()},\n",
    "    }\n",
    "\n",
    "    metadata_path = ctx.get(\"outputs\", {}).get(\"metadata\")\n",
    "    if metadata_path is None:\n",
    "        metadata_path = ctx[\"variant_root\"] / f\"{phase}_metadata.json\"\n",
    "    write_metadata(\n",
    "        stage=phase,\n",
    "        variant=variant,\n",
    "        parent_variant=parent_variant,\n",
    "        inputs=[str(parent_dataset)],\n",
    "        outputs=[str(model_final_path), str(experiments_dir / \"trials.parquet\")],\n",
    "        params=params,\n",
    "        metadata_path=metadata_path,\n",
    "    )\n",
    "\n",
    "    print(\"[OK] Entrenamiento completado\")\n",
    "    print(f\"[OK] Modelo final: {model_final_path}\")\n",
    "    print(f\"[OK] Trials: {experiments_dir / 'trials.parquet'}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74feb37",
   "metadata": {},
   "source": [
    "## 2) Ejecutar\n",
    "\n",
    "Define la variante (por defecto `v501`) y ejecuta `main(VARIANT)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfc190c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T17:57:55.415327Z",
     "iopub.status.busy": "2026-02-18T17:57:55.415130Z",
     "iopub.status.idle": "2026-02-18T17:58:55.524062Z",
     "shell.execute_reply": "2026-02-18T17:58:55.523439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTX] execution_dir: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/notebooks\n",
      "[CTX] project_root: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp\n",
      "[CTX] phase: 05_modeling\n",
      "[CTX] variant: v504\n",
      "[CTX] variant_root: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/05_modeling/v504\n",
      "[CTX] figures_dir: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/05_modeling/v504/figures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Entrenamiento completado\n",
      "[OK] Modelo final: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/05_modeling/v504/model_final.h5\n",
      "[OK] Trials: /Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/executions/05_modeling/v504/experiments/trials.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancarlosduenaslopez/Documents/mlops/mlops4ofp/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EJECUCIÓN\n",
    "# ============================================================\n",
    "\n",
    "# Variante a ejecutar.\n",
    "# - Si ejecutas con Makefile (nb5-run), puedes exportar VARIANT en entorno.\n",
    "# - Si ejecutas manualmente en VSCode/Jupyter, cambia este valor.\n",
    "VARIANT = os.environ.get(\"ACTIVE_VARIANT\", \"v501\")\n",
    "\n",
    "main(VARIANT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
